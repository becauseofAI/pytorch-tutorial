<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="becauseofAI">
        <link rel="canonical" href="https://becauseofAI.github.io/pytorch-tutorial/chapter02_basics/2_1_pytorch-basics-tensor/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>2.1 Tensor - PyTorch Tutorial</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="../../css/extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-27795084-5', 'mkdocs.org');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">PyTorch Tutorial</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Introduction</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorials <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
  <li class="dropdown-submenu">
    <a href="#">Ghapter01 Getting Started</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../chapter01_getting-started/1_1_pytorch-introduction/">1.1 PyTorch Tntroduction</a>
</li>
            
<li >
    <a href="../../chapter01_getting-started/1_3_deep-learning-with-pytorch-60-minute-blitz/">1.3 PyTorch 60 Minute Blitz</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Ghapter02 Basics</a>
    <ul class="dropdown-menu">
            
<li class="active">
    <a href="./">2.1 Tensor</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../about/">About</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../../chapter01_getting-started/1_3_deep-learning-with-pytorch-60-minute-blitz/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../../about/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/becauseofAI/pytorch-tutorial/">pytorch-tutorial</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#pytorch">PyTorch 基础 : 张量</a></li>
            <li><a href="#tensor">张量(Tensor)</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="pytorch">PyTorch 基础 : 张量<a class="headerlink" href="#pytorch" title="Permanent link">&para;</a></h1>
<p>在第一章中我们已经通过官方的入门教程对PyTorch有了一定的了解，这一章会详细介绍PyTorch 里面的基础知识。
全部掌握了这些基础知识，在后面的应用中才能更加快速进阶，如果你已经对PyTorch有一定的了解，可以跳过此章</p>
<div class="codehilite"><pre><span></span><span class="c1"># 首先要引入相关的包</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c1">#打印一下版本</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

<div class="codehilite"><pre><span></span>&#39;1.0.0&#39;
</pre></div>


<h2 id="tensor">张量(Tensor)<a class="headerlink" href="#tensor" title="Permanent link">&para;</a></h2>
<p>张量的英文是Tensor，它是PyTorch里面基础的运算单位,与Numpy的ndarray相同都表示的是一个多维的矩阵。
与ndarray的最大区别就是，PyTorch的Tensor可以在 GPU 上运行，而 numpy 的 ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。</p>
<p>下面我们生成一个简单的张量</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[0.6904, 0.7419, 0.8010],
        [0.1722, 0.2442, 0.8181]])
</pre></div>


<p>以上生成了一个，2行3列的的矩阵，我们看一下他的大小：</p>
<div class="codehilite"><pre><span></span><span class="c1"># 可以使用与numpy相同的shape属性查看</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 也可以使用size()函数，返回的结果都是相同的</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>

<div class="codehilite"><pre><span></span>torch.Size([2, 3])
torch.Size([2, 3])
</pre></div>


<p>张量（Tensor）是一个定义在一些向量空间和一些对偶空间的笛卡儿积上的多重线性映射，其坐标是|n|维空间内，有|n|个分量的一种量， 其中每个分量都是坐标的函数， 而在坐标变换时，这些分量也依照某些规则作线性变换。r称为该张量的秩或阶（与矩阵的秩和阶均无关系）。 (来自百度百科)</p>
<p>下面我们来生成一些多维的张量：</p>
<div class="codehilite"><pre><span></span><span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">y</span>
</pre></div>

<div class="codehilite"><pre><span></span>torch.Size([2, 3, 4, 5])





tensor([[[[0.9071, 0.0616, 0.0006, 0.6031, 0.0714],
          [0.6592, 0.9700, 0.0253, 0.0726, 0.5360],
          [0.5416, 0.1138, 0.9592, 0.6779, 0.6501],
          [0.0546, 0.8287, 0.7748, 0.4352, 0.9232]],

         [[0.0730, 0.4228, 0.7407, 0.4099, 0.1482],
          [0.5408, 0.9156, 0.6554, 0.5787, 0.9775],
          [0.4262, 0.3644, 0.1993, 0.4143, 0.5757],
          [0.9307, 0.8839, 0.8462, 0.0933, 0.6688]],

         [[0.4447, 0.0929, 0.9882, 0.5392, 0.1159],
          [0.4790, 0.5115, 0.4005, 0.9486, 0.0054],
          [0.8955, 0.8097, 0.1227, 0.2250, 0.5830],
          [0.8483, 0.2070, 0.1067, 0.4727, 0.5095]]],


        [[[0.9438, 0.2601, 0.2885, 0.5457, 0.7528],
          [0.2971, 0.2171, 0.3910, 0.1924, 0.2570],
          [0.7491, 0.9749, 0.2703, 0.2198, 0.9472],
          [0.1216, 0.6647, 0.8809, 0.0125, 0.5513]],

         [[0.0870, 0.6622, 0.7252, 0.4783, 0.0160],
          [0.7832, 0.6050, 0.7469, 0.7947, 0.8052],
          [0.1755, 0.4489, 0.0602, 0.8073, 0.3028],
          [0.9937, 0.6780, 0.9425, 0.0059, 0.0451]],

         [[0.3851, 0.8742, 0.5932, 0.4899, 0.8354],
          [0.8577, 0.3705, 0.0229, 0.7097, 0.7557],
          [0.1505, 0.3527, 0.0843, 0.0088, 0.8741],
          [0.6041, 0.8797, 0.6189, 0.9495, 0.1479]]]])
</pre></div>


<p>在同构的意义下，第零阶张量 （r = 0） 为标量 （Scalar），第一阶张量 （r = 1） 为向量 （Vector）， 第二阶张量 （r = 2） 则成为矩阵 （Matrix），第三阶以上的统称为多维张量。</p>
<p>其中要特别注意的就是标量，我们先生成一个标量：</p>
<div class="codehilite"><pre><span></span><span class="c1">#我们直接使用现有数字生成</span>
<span class="n">scalar</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1433223</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scalar</span><span class="p">)</span>
<span class="c1">#打印标量的大小</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor(3.1433)





torch.Size([])
</pre></div>


<p>对于标量，我们可以直接使用 .item() 从中取出其对应的python对象的数值</p>
<div class="codehilite"><pre><span></span><span class="n">scalar</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>3.143322229385376
</pre></div>


<p>特别的：如果张量中只有一个元素的tensor也可以调用<code>tensor.item</code>方法</p>
<div class="codehilite"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.1433223</span><span class="p">])</span> 
<span class="k">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3.1433])





torch.Size([1])
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>3.143322229385376
</pre></div>


<h3 id="_1">基本类型<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>Tensor的基本数据类型有五种：
- 32位浮点型：torch.FloatTensor。 (默认)
- 64位整型：torch.LongTensor。
- 32位整型：torch.IntTensor。
- 16位整型：torch.ShortTensor。
- 64位浮点型：torch.DoubleTensor。</p>
<p>除以上数字类型外，还有
byte和chart型</p>
<div class="codehilite"><pre><span></span><span class="nb">long</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="nb">long</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3])
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">half</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="n">half</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3.1426], dtype=torch.float16)
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">int_t</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
<span class="n">int_t</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3], dtype=torch.int32)
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">flo</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">flo</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3.1433])
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">short</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">short</span><span class="p">()</span>
<span class="n">short</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3], dtype=torch.int16)
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">ch</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">char</span><span class="p">()</span>
<span class="n">ch</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3], dtype=torch.int8)
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">bt</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span>
<span class="n">bt</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([3], dtype=torch.uint8)
</pre></div>


<h3 id="numpy">Numpy转换<a class="headerlink" href="#numpy" title="Permanent link">&para;</a></h3>
<p>使用numpy方法将Tensor转为ndarray</p>
<div class="codehilite"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># tensor转化为numpy</span>
<span class="n">numpy_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">numpy_a</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>[[ 0.46819344  1.3774964 ]
 [ 0.9491934   1.4543315 ]
 [-0.42792308  0.99790514]]
</pre></div>


<p>numpy转化为Tensor</p>
<div class="codehilite"><pre><span></span><span class="n">torch_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">numpy_a</span><span class="p">)</span>
<span class="n">torch_a</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[ 0.4682,  1.3775],
        [ 0.9492,  1.4543],
        [-0.4279,  0.9979]])
</pre></div>


<p><strong><em>Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。</em></strong></p>
<h3 id="_2">设备间转换<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<p>一般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持</p>
<div class="codehilite"><pre><span></span><span class="n">cpu_a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">cpu_a</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>&#39;torch.FloatTensor&#39;
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">gpu_a</span><span class="o">=</span><span class="n">cpu_a</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">gpu_a</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>&#39;torch.cuda.FloatTensor&#39;
</pre></div>


<p>使用.cpu方法将tensor移动到cpu</p>
<div class="codehilite"><pre><span></span><span class="n">cpu_b</span><span class="o">=</span><span class="n">gpu_a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">cpu_b</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>&#39;torch.FloatTensor&#39;
</pre></div>


<p>如果我们有多GPU的情况，可以使用to方法来确定使用那个设备，这里只做个简单的实例：</p>
<div class="codehilite"><pre><span></span><span class="c1">#使用torch.cuda.is_available()来确定是否有cuda设备</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">#将tensor传送到设备</span>
<span class="n">gpu_b</span><span class="o">=</span><span class="n">cpu_b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">gpu_b</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

<div class="codehilite"><pre><span></span>cuda





&#39;torch.cuda.FloatTensor&#39;
</pre></div>


<h3 id="_3">初始化<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>Pytorch中有许多默认的初始化方法可以使用</p>
<div class="codehilite"><pre><span></span><span class="c1"># 使用[0,1]均匀分布随机初始化二维数组</span>
<span class="n">rnd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">rnd</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[0.3804, 0.0297, 0.5241],
        [0.4111, 0.8887, 0.4642],
        [0.7302, 0.5913, 0.7182],
        [0.3048, 0.8055, 0.2176],
        [0.6195, 0.1620, 0.7726]])
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1">##初始化，使用1填充</span>
<span class="n">one</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">one</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[1., 1.],
        [1., 1.]])
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1">##初始化，使用0填充</span>
<span class="n">zero</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">zero</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[0., 0.],
        [0., 0.]])
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1">#初始化一个单位矩阵，即对角线为1 其他为0</span>
<span class="n">eye</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">eye</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[1., 0.],
        [0., 1.]])
</pre></div>


<h3 id="_4">常用方法<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>PyTorch中对张量的操作api 和 NumPy 非常相似，如果熟悉 NumPy 中的操作，那么 他们二者 基本是一致的：</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[ 0.6922, -0.4824,  0.8594],
        [ 0.4509, -0.8155, -0.0368],
        [ 1.3533,  0.5545, -0.0509]])
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1"># 沿着行取最大值</span>
<span class="n">max_value</span><span class="p">,</span> <span class="n">max_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">max_value</span><span class="p">,</span> <span class="n">max_idx</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([0.8594, 0.4509, 1.3533]) tensor([2, 0, 0])
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1"># 每行 x 求和</span>
<span class="n">sum_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sum_x</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([ 1.0692, -0.4014,  1.8568])
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[-0.3821, -2.6932, -1.3884],
        [ 0.7468, -0.7697, -0.0883],
        [ 0.7688, -1.3485,  0.7517]])
</pre></div>


<p>正如官方60分钟教程中所说，以_为结尾的，均会改变调用值</p>
<div class="codehilite"><pre><span></span><span class="c1"># add 完成后x的值改变了</span>
<span class="n">x</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>tensor([[-0.3821, -2.6932, -1.3884],
        [ 0.7468, -0.7697, -0.0883],
        [ 0.7688, -1.3485,  0.7517]])
</pre></div>


<p>张量的基本操作都介绍的的差不多了，下一章介绍PyTorch的自动求导机制</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2019 <a href="https://github.com/becauseofAI">becauseofAI</a>, Maintained by the <a href="https://github.com/becauseofAI">becauseofAI</a>.</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../js/extra.js" defer></script>
        <script src="../../js/baidu-tongji.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
