



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="PyTorch Tutorial for Deep Learning Research and Product.">
      
      
        <link rel="canonical" href="https://becauseofAI.github.io/pytorch-tutorial/tutorial/chapter02_basics/2_2_deep-learning-mathematics-basic/">
      
      
        <meta name="author" content="becauseofAI">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="ja">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.0">
    
    
      
        <title>2.2 Deep Learning Mathematics Basic - PyTorch Tutorial</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.0284f74d.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.01803549.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
      <script src="../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../../css/extra.css">
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-27795084-5", "mkdocs.org")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="red" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#22" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://becauseofAI.github.io/pytorch-tutorial/" title="PyTorch Tutorial" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              PyTorch Tutorial
            </span>
            <span class="md-header-nav__topic">
              
                2.2 Deep Learning Mathematics Basic
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/becauseofAI/pytorch-tutorial/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    pytorch-tutorial
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://becauseofAI.github.io/pytorch-tutorial/" title="PyTorch Tutorial" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch Tutorial
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/becauseofAI/pytorch-tutorial/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    pytorch-tutorial
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      Ghapter01 Getting Started
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        Ghapter01 Getting Started
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter01_getting-started/1_1_pytorch-introduction/" title="1.1 PyTorch Tntroduction" class="md-nav__link">
      1.1 PyTorch Tntroduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-1-2" type="checkbox" id="nav-2-1-2">
    
    <label class="md-nav__link" for="nav-2-1-2">
      1.3 PyTorch 60 Minute Blitz
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-1-2">
        1.3 PyTorch 60 Minute Blitz
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter01_getting-started/1_3_1_tensor_tutorial/" title="1.3.1 Tensor" class="md-nav__link">
      1.3.1 Tensor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter01_getting-started/1_3_2_autograd_tutorial/" title="1.3.2 Autograd" class="md-nav__link">
      1.3.2 Autograd
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter01_getting-started/1_3_3_neural_networks_tutorial/" title="1.3.3 Neural Networks" class="md-nav__link">
      1.3.3 Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter01_getting-started/1_3_4_cifar10_tutorial/" title="1.3.4 Classifier" class="md-nav__link">
      1.3.4 Classifier
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter01_getting-started/1_3_5_data_parallel_tutorial/" title="1.3.5 Data Parallelism" class="md-nav__link">
      1.3.5 Data Parallelism
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2" checked>
    
    <label class="md-nav__link" for="nav-2-2">
      Ghapter02 Basics
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        Ghapter02 Basics
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2-1" type="checkbox" id="nav-2-2-1">
    
    <label class="md-nav__link" for="nav-2-2-1">
      2.1 Basic
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-2-1">
        2.1 Basic
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../2_1_1_pytorch-basics-tensor/" title="2.1.1 Tensor" class="md-nav__link">
      2.1.1 Tensor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2_1_2_pytorch-basics-autograd/" title="2.1.2 AutoGrad" class="md-nav__link">
      2.1.2 AutoGrad
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2_1_3_pytorch-basics-nerual-network/" title="2.1.3 Nerual Network" class="md-nav__link">
      2.1.3 Nerual Network
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2_1_4_pytorch-basics-data-loader/" title="2.1.4 Data Loader" class="md-nav__link">
      2.1.4 Data Loader
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        2.2 Deep Learning Mathematics Basic
      </label>
    
    <a href="./" title="2.2 Deep Learning Mathematics Basic" class="md-nav__link md-nav__link--active">
      2.2 Deep Learning Mathematics Basic
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#221" title="2.2.1 监督学习和无监督学习" class="md-nav__link">
    2.2.1 监督学习和无监督学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#222-linear-regreesion" title="2.2.2 线性回归 （Linear Regreesion）" class="md-nav__link">
    2.2.2 线性回归 （Linear Regreesion）
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#223-loss-function" title="2.2.3 损失函数(Loss Function)" class="md-nav__link">
    2.2.3 损失函数(Loss Function)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nnl1loss" title="nn.L1Loss:" class="md-nav__link">
    nn.L1Loss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnnllloss" title="nn.NLLLoss:" class="md-nav__link">
    nn.NLLLoss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnmseloss" title="nn.MSELoss:" class="md-nav__link">
    nn.MSELoss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nncrossentropyloss" title="nn.CrossEntropyLoss:" class="md-nav__link">
    nn.CrossEntropyLoss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnbceloss" title="nn.BCELoss:" class="md-nav__link">
    nn.BCELoss:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#224" title="2.2.4 梯度下降" class="md-nav__link">
    2.2.4 梯度下降
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="梯度" class="md-nav__link">
    梯度
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="梯度下降法直观解释" class="md-nav__link">
    梯度下降法直观解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mini-batch" title="Mini-batch的梯度下降法" class="md-nav__link">
    Mini-batch的梯度下降法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptimsgd" title="torch.optim.SGD" class="md-nav__link">
    torch.optim.SGD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptimrmsprop" title="torch.optim.RMSprop" class="md-nav__link">
    torch.optim.RMSprop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptimadam" title="torch.optim.Adam" class="md-nav__link">
    torch.optim.Adam
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#225" title="2.2.5 方差/偏差" class="md-nav__link">
    2.2.5 方差/偏差
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#226" title="2.2.6 正则化" class="md-nav__link">
    2.2.6 正则化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1" title="L1正则化" class="md-nav__link">
    L1正则化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l2" title="L2正则化" class="md-nav__link">
    L2正则化
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2_3_deep-learning-neural-network-introduction/" title="2.3 Deep Learning Neural Network Introduction" class="md-nav__link">
      2.3 Deep Learning Neural Network Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2_4_convolutional-neural-network/" title="2.4 Convolutional Neural Network" class="md-nav__link">
      2.4 Convolutional Neural Network
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../2_5_recurrent-neural-network/" title="2.5 Recurrent Neural Network" class="md-nav__link">
      2.5 Recurrent Neural Network
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-3" type="checkbox" id="nav-2-3">
    
    <label class="md-nav__link" for="nav-2-3">
      Ghapter03 Intermediate
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-3">
        Ghapter03 Intermediate
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter03_intermediate/3_1_logistic-regression/" title="3.1 Logistic Regression" class="md-nav__link">
      3.1 Logistic Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-3-2" type="checkbox" id="nav-2-3-2">
    
    <label class="md-nav__link" for="nav-2-3-2">
      3.2 CNN
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-3-2">
        3.2 CNN
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter03_intermediate/3_2_1_cnn_convnet_mnist/" title="3.2.1 ConvNet Mnist" class="md-nav__link">
      3.2.1 ConvNet Mnist
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter03_intermediate/3_2_2_cnn_resnet_cifar10/" title="3.2.2 ResNet_Cifar10" class="md-nav__link">
      3.2.2 ResNet_Cifar10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter03_intermediate/3_3_rnn/" title="3.3 RNN" class="md-nav__link">
      3.3 RNN
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-4" type="checkbox" id="nav-2-4">
    
    <label class="md-nav__link" for="nav-2-4">
      Ghapter04 Advanced
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-4">
        Ghapter04 Advanced
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter04_advanced/4_1_fine-tuning/" title="Fine Tuning" class="md-nav__link">
      Fine Tuning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-4-2" type="checkbox" id="nav-2-4-2">
    
    <label class="md-nav__link" for="nav-2-4-2">
      Visualization
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-4-2">
        Visualization
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter04_advanced/4_2_1_visdom/" title="Visdom" class="md-nav__link">
      Visdom
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter04_advanced/4_2_2_tensorboardx/" title="TensorBoardX" class="md-nav__link">
      TensorBoardX
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter04_advanced/4_2_3_cnn-visualizing/" title="CNN Visualizing" class="md-nav__link">
      CNN Visualizing
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter04_advanced/4_3_multiply-gpu-parallel-training/" title="Parallel" class="md-nav__link">
      Parallel
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter04_advanced/4_4_fastai/" title="FastAI" class="md-nav__link">
      FastAI
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-5" type="checkbox" id="nav-2-5">
    
    <label class="md-nav__link" for="nav-2-5">
      Ghapter05 Application
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-5">
        Ghapter05 Application
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/5_1_kaggle/" title="5.1 Kaggle" class="md-nav__link">
      5.1 Kaggle
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="5.2 结构化数据" class="md-nav__link">
      5.2 结构化数据
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-5-3" type="checkbox" id="nav-2-5-3">
    
    <label class="md-nav__link" for="nav-2-5-3">
      5.3 Computer Vision
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-5-3">
        5.3 Computer Vision
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="Detection" class="md-nav__link">
      Detection
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="Segmentation" class="md-nav__link">
      Segmentation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="Recognition" class="md-nav__link">
      Recognition
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="GAN" class="md-nav__link">
      GAN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="Others" class="md-nav__link">
      Others
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="5.4 自然语言处理" class="md-nav__link">
      5.4 自然语言处理
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../chapter05_application/readme/" title="5.5 协同过滤" class="md-nav__link">
      5.5 协同过滤
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../about/" title="About" class="md-nav__link">
      About
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#221" title="2.2.1 监督学习和无监督学习" class="md-nav__link">
    2.2.1 监督学习和无监督学习
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#222-linear-regreesion" title="2.2.2 线性回归 （Linear Regreesion）" class="md-nav__link">
    2.2.2 线性回归 （Linear Regreesion）
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#223-loss-function" title="2.2.3 损失函数(Loss Function)" class="md-nav__link">
    2.2.3 损失函数(Loss Function)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nnl1loss" title="nn.L1Loss:" class="md-nav__link">
    nn.L1Loss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnnllloss" title="nn.NLLLoss:" class="md-nav__link">
    nn.NLLLoss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnmseloss" title="nn.MSELoss:" class="md-nav__link">
    nn.MSELoss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nncrossentropyloss" title="nn.CrossEntropyLoss:" class="md-nav__link">
    nn.CrossEntropyLoss:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnbceloss" title="nn.BCELoss:" class="md-nav__link">
    nn.BCELoss:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#224" title="2.2.4 梯度下降" class="md-nav__link">
    2.2.4 梯度下降
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="梯度" class="md-nav__link">
    梯度
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="梯度下降法直观解释" class="md-nav__link">
    梯度下降法直观解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mini-batch" title="Mini-batch的梯度下降法" class="md-nav__link">
    Mini-batch的梯度下降法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptimsgd" title="torch.optim.SGD" class="md-nav__link">
    torch.optim.SGD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptimrmsprop" title="torch.optim.RMSprop" class="md-nav__link">
    torch.optim.RMSprop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptimadam" title="torch.optim.Adam" class="md-nav__link">
    torch.optim.Adam
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#225" title="2.2.5 方差/偏差" class="md-nav__link">
    2.2.5 方差/偏差
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#226" title="2.2.6 正则化" class="md-nav__link">
    2.2.6 正则化
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1" title="L1正则化" class="md-nav__link">
    L1正则化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l2" title="L2正则化" class="md-nav__link">
    L2正则化
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="22">2.2 深度学习基础及数学原理<a class="headerlink" href="#22" title="Permanent link">&para;</a></h1>
<p>深度学习并没有想象的那么难，甚至比有些传统的机器学习更简单。所用到的数学知识也不需要特别的高深，本章将会一边讲解深度学习中的基本理论，一边通过动手使用PyTorch实现一些简单的理论，本章内容很多，所以只做一个简短的介绍</p>
<h2 id="221">2.2.1 监督学习和无监督学习<a class="headerlink" href="#221" title="Permanent link">&para;</a></h2>
<p>监督学习、无监督学习、半监督学习、强化学习是我们日常接触到的常见的四个机器学习方法：</p>
<ul>
<li>监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。</li>
<li>无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。 </li>
<li>半监督学习 ：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。</li>
<li>强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。</li>
</ul>
<p>这里我们只着重介绍监督学习，因为我们后面的绝大部们课程都是使用的监督学习的方法，在训练和验证时输入的数据既包含输入x,又包含x对应的输出y，即学习数据已经事先给出了正确答案。</p>
<h2 id="222-linear-regreesion">2.2.2 线性回归 （Linear Regreesion）<a class="headerlink" href="#222-linear-regreesion" title="Permanent link">&para;</a></h2>
<p>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。 </p>
<p>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。
摘自<a href="https://baike.baidu.com/item/线性回归/8190345">百度百科</a></p>
<p>简单的说：
线性回归对于输入x与输出y有一个映射f，y=f(x),而f的形式为aX+b。其中a和b是两个可调的参数，我们训练的时候就是训练a，b这两个参数。</p>
<p>下面我们来用pyTorch的代码来做一个详细的解释:</p>
<div class="codehilite"><pre><span></span><span class="c1"># 引用</span>
<span class="c1"># 注意，这里我们使用了一个新库叫 seaborn 如果报错找不到包的话请使用pip install seaborn 来进行安装</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Module</span><span class="p">,</span> <span class="n">MSELoss</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

<div class="codehilite"><pre><span></span>&#39;1.0.1.post2&#39;
</pre></div>


<p>下面定义一个线性函数，这里使用 <span><span class="MathJax_Preview">y = 5x + 7</span><script type="math/tex">y = 5x + 7</script></span>，这里的5和7就是上面说到的参数a和b，我们先使用matplot可视化一下这个函数</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">7</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>

<div class="codehilite"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7fd40bbe57f0&gt;]
</pre></div>


<p><img alt="png" src="../img/2_2_deep-learning-mathematics-basic_6_1.png" /></p>
<p>下面我生成一些随机的点，来作为我们的训练数据</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">7</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>

<p>在图上显示下我们生成的数据</p>
<div class="codehilite"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">);</span>
</pre></div>

<p><img alt="png" src="../img/2_2_deep-learning-mathematics-basic_10_0.png" /></p>
<p>我们随机生成了一些点，下面将使用PyTorch建立一个线性的模型来对其进行拟合，这就是所说的训练的过程，由于只有一层线性模型，所以我们就直接使用了。</p>
<div class="codehilite"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>

<p>其中参数(1, 1)代表输入输出的特征(feature)数量都是1. <code>Linear</code> 模型的表达式是 <span><span class="MathJax_Preview">y=w \cdot x+b</span><script type="math/tex">y=w \cdot x+b</script></span>, 其中 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 代表权重, <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 代表偏置。</p>
<p>损失函数我们使用均方损失函数：<code>MSELoss</code>，这个后面会详细介绍。</p>
<div class="codehilite"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
</pre></div>

<p>优化器我们选择最常见的优化方法 <code>SGD</code>，就是每一次迭代计算 <code>mini-batch</code> 的梯度，然后对参数进行更新，学习率 0.01 ，优化器本章后面也会进行介绍。</p>
<div class="codehilite"><pre><span></span><span class="n">optim</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">)</span>
</pre></div>

<p>训练3000次</p>
<div class="codehilite"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">3000</span>
</pre></div>

<p>准备训练数据: <code>x_train</code>, <code>y_train</code> 的形状是 (256, 1), 代表 <code>mini-batch</code> 大小为256, <code>feature</code> 为1. <code>astype('float32')</code> 是为了下一步可以直接转换为 <code>torch.float</code>.</p>
<div class="codehilite"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>

<p>开始训练：
<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="c1">#使用模型进行预测</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="c1">#梯度置0，否则会累加</span>
    <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># 计算损失</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="c1"># 反向传播</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># 使用优化器默认方法优化</span>
    <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">%</span><span class="mi">100</span><span class="o">==</span><span class="mi">0</span><span class="p">):</span>
        <span class="c1">#每 100次打印一下损失函数，看看效果</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;epoch {}, loss {:1.4f}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>       
</pre></div></p>
<div class="codehilite"><pre><span></span>epoch 0, loss 105.8649
epoch 100, loss 0.7534
epoch 200, loss 0.1216
epoch 300, loss 0.1029
epoch 400, loss 0.0915
epoch 500, loss 0.0828
epoch 600, loss 0.0763
epoch 700, loss 0.0713
epoch 800, loss 0.0675
epoch 900, loss 0.0647
epoch 1000, loss 0.0625
epoch 1100, loss 0.0608
epoch 1200, loss 0.0596
epoch 1300, loss 0.0586
epoch 1400, loss 0.0579
epoch 1500, loss 0.0574
epoch 1600, loss 0.0570
epoch 1700, loss 0.0566
epoch 1800, loss 0.0564
epoch 1900, loss 0.0562
epoch 2000, loss 0.0561
epoch 2100, loss 0.0560
epoch 2200, loss 0.0559
epoch 2300, loss 0.0558
epoch 2400, loss 0.0558
epoch 2500, loss 0.0558
epoch 2600, loss 0.0557
epoch 2700, loss 0.0557
epoch 2800, loss 0.0557
epoch 2900, loss 0.0557
</pre></div>


<p>训练完成了，看一下训练的成果是多少。 用 <code>model.parameters()</code> 提取模型参数。 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>, <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 是我们所需要训练的模型参数。
我们期望的数据 <span><span class="MathJax_Preview">w=5</span><script type="math/tex">w=5</script></span>，<span><span class="MathJax_Preview">b=7</span><script type="math/tex">b=7</script></span> 可以做一下对比：</p>
<div class="codehilite"><pre><span></span><span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="k">print</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>

<div class="codehilite"><pre><span></span>4.994358062744141 7.0252156257629395
</pre></div>


<p>再次可视化一下我们的模型，看看我们训练的数据，如果你不喜欢seaborn，可以直接使用matplot：</p>
<div class="codehilite"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;go&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;predicted&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

<p><img alt="png" src="../img/2_2_deep-learning-mathematics-basic_27_0.png" /></p>
<p>以上就是一个使用PyTorch做线性回归的简单样例了，下面我们会对上面的内容做详细的介绍</p>
<h2 id="223-loss-function">2.2.3 损失函数(Loss Function)<a class="headerlink" href="#223-loss-function" title="Permanent link">&para;</a></h2>
<p>损失函数（loss function）是用来估量模型的预测值(我们例子中的output)与真实值（例子中的y_train）的不一致程度，它是一个非负实值函数,损失函数越小，模型的鲁棒性就越好。
我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。</p>
<p>这里有一个重点：因为PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均</p>
<p>常见（PyTorch内置）的损失函数有以下几个：</p>
<h3 id="nnl1loss">nn.L1Loss:<a class="headerlink" href="#nnl1loss" title="Permanent link">&para;</a></h3>
<p>输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的</p>
<div>
<div class="MathJax_Preview"> loss(x,y)=1/n\sum|x_i-y_i| </div>
<script type="math/tex; mode=display"> loss(x,y)=1/n\sum|x_i-y_i| </script>
</div>
<h3 id="nnnllloss">nn.NLLLoss:<a class="headerlink" href="#nnnllloss" title="Permanent link">&para;</a></h3>
<p>用于多分类的负对数似然损失函数</p>
<div>
<div class="MathJax_Preview"> loss(x, class) = -x[class] </div>
<script type="math/tex; mode=display"> loss(x, class) = -x[class] </script>
</div>
<p>NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了</p>
<div>
<div class="MathJax_Preview"> loss(x, class) = -weights[class] * x[class] </div>
<script type="math/tex; mode=display"> loss(x, class) = -weights[class] * x[class] </script>
</div>
<h3 id="nnmseloss">nn.MSELoss:<a class="headerlink" href="#nnmseloss" title="Permanent link">&para;</a></h3>
<p>均方损失函数 ，输入x和目标y之间均方差</p>
<div>
<div class="MathJax_Preview"> loss(x,y)=1/n\sum(x_i-y_i)^2 </div>
<script type="math/tex; mode=display"> loss(x,y)=1/n\sum(x_i-y_i)^2 </script>
</div>
<h3 id="nncrossentropyloss">nn.CrossEntropyLoss:<a class="headerlink" href="#nncrossentropyloss" title="Permanent link">&para;</a></h3>
<p>多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数,我们可以理解为CrossEntropyLoss() = log_softmax() + NLLLoss()</p>
<p>$$ \begin{aligned} loss(x, class) &amp;= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))} &amp;= -x[class] + log(\sum_j exp(x[j])) \end{aligned}  $$</p>
<p>因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：</p>
<p>$$ loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j]))) $$</p>
<p>所以一般多分类的情况会使用这个损失函数。</p>
<h3 id="nnbceloss">nn.BCELoss:<a class="headerlink" href="#nnbceloss" title="Permanent link">&para;</a></h3>
<p>计算 x 与 y 之间的二进制交叉熵。</p>
<div>
<div class="MathJax_Preview"> loss(o,t) = -\frac{1}{n}\sum_i(t[i] * log(o[i])+(1-t[i]) * log(1-o[i])) </div>
<script type="math/tex; mode=display"> loss(o,t) = -\frac{1}{n}\sum_i(t[i] * log(o[i])+(1-t[i]) * log(1-o[i])) </script>
</div>
<p>与NLLLoss类似，也可以添加权重参数： </p>
<div>
<div class="MathJax_Preview"> loss(o,t)=-\frac{1}{n}\sum_iweights[i] * (t[i] * log(o[i])+(1-t[i]) * log(1-o[i])) </div>
<script type="math/tex; mode=display"> loss(o,t)=-\frac{1}{n}\sum_iweights[i] * (t[i] * log(o[i])+(1-t[i]) * log(1-o[i])) </script>
</div>
<p>用的时候需要在该层前面加上 Sigmoid 函数。</p>
<h2 id="224">2.2.4 梯度下降<a class="headerlink" href="#224" title="Permanent link">&para;</a></h2>
<p>在介绍损失函数的时候我们已经说了，梯度下降是一个使损失函数越来越小的优化算法，在无求解机器学习算法的模型参数，即约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。所以梯度下降是我们目前所说的机器学习的核心，了解了它的含义，也就了解了机器学习算法的含义。</p>
<h3 id="_1">梯度<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。
例如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。</p>
<p>几何上讲，梯度就是函数变化增加最快的地方，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向梯度减少最快，也就是更加容易找到函数的最小值。</p>
<p>我们需要最小化损失函数，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。</p>
<h3 id="_2">梯度下降法直观解释<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<p>梯度下降法就好比下山，我们并不知道下山的路，于是决定走一步算一步，每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。</p>
<p>如下图所示，（此图摘自百度百科）  </p>
<p><img alt="" src="../img/1.png" />  </p>
<p>这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处（局部最优解）。</p>
<p>这个问题在以前的机器学习中可能会遇到，因为机器学习中的特征比较少，所以导致很可能陷入到一个局部最优解中出不来，但是到了深度学习，动辄百万甚至上亿的特征，出现这种情况的概率几乎为0，所以我们可以不用考虑这个问题。</p>
<h3 id="mini-batch">Mini-batch的梯度下降法<a class="headerlink" href="#mini-batch" title="Permanent link">&para;</a></h3>
<p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或者显存中，所以我们会把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为Mini-batch。
在PyTorch中就是使用这种方法进行的训练，可以看看上一章中关于dataloader的介绍里面的batch_size就是我们一个Mini-batch的大小。</p>
<p>为了介绍的更简洁，使用 吴恩达老师的 <a href="https://www.deeplearning.ai/deep-learning-specialization/">deeplearning.ai</a> 课程板书。</p>
<p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。  </p>
<p><img alt="" src="../img/2.png" />  </p>
<p>普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示：  </p>
<p><img alt="" src="../img/3.png" />  </p>
<ul>
<li>如果训练样本的大小比较小时,能够一次性的读取到内存中，那我们就不需要使用Mini-batch，</li>
<li>如果训练样本的大小比较大时，一次读入不到内存或者现存中，那我们必须要使用 Mini-batch来分批的计算</li>
<li>Mini-batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size  </li>
</ul>
<p><img alt="" src="../img/4.png" />  </p>
<p><code>torch.optim</code>是一个实现了各种优化算法的库。大部分常用优化算法都有实现，我们直接调用即可。</p>
<h3 id="torchoptimsgd">torch.optim.SGD<a class="headerlink" href="#torchoptimsgd" title="Permanent link">&para;</a></h3>
<p>随机梯度下降算法,带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：</p>
<div class="codehilite"><pre><span></span><span class="c1">#lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中详细说明</span>
<span class="c1">##如果设置了momentum，就是带有动量的SGD，可以不设置</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>

<h3 id="torchoptimrmsprop">torch.optim.RMSprop<a class="headerlink" href="#torchoptimrmsprop" title="Permanent link">&para;</a></h3>
<p>除了以上的带有动量Momentum梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快</p>
<div class="codehilite"><pre><span></span><span class="c1">#我们的课程基本不会使用到RMSprop所以这里只给一个实例</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
</pre></div>

<h3 id="torchoptimadam">torch.optim.Adam<a class="headerlink" href="#torchoptimadam" title="Permanent link">&para;</a></h3>
<p>Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法</p>
<div class="codehilite"><pre><span></span><span class="c1"># 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">)</span>
</pre></div>

<h2 id="225">2.2.5 方差/偏差<a class="headerlink" href="#225" title="Permanent link">&para;</a></h2>
<ul>
<li>偏差度量了学习算法的期望预测与真实结果的偏离程序, 即 刻画了学习算法本身的拟合能力</li>
<li>方差度量了同样大小的训练集的变动所导致的学习性能的变化, 即**模型的泛化能力**：  </li>
</ul>
<p><img alt="" src="../img/5.png" />  </p>
<p>从图中我们可以看出：
- 高偏差（high bias）的情况，一般称为欠拟合（underfitting）,即我们的模型并没有很好的去适配现有的数据，拟合度不够。
- 高方差（high variance）的情况一般称作过拟合（overfitting），即模型对于训练数据拟合度太高了，失去了泛化的能力。</p>
<p>如何解决这两种情况呢？</p>
<p>欠拟合：
- 增加网络结构，如增加隐藏层数目；
- 训练更长时间；
- 寻找合适的网络架构，使用更大的NN结构；</p>
<p>过拟合 ：
- 使用更多的数据；
- 正则化（ regularization）；
- 寻找合适的网络结构；</p>
<p>例如我们上面的例子，可以计算出我们的偏差:</p>
<div class="codehilite"><pre><span></span><span class="k">print</span> <span class="p">(</span><span class="mi">5</span><span class="o">-</span><span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="mi">7</span><span class="o">-</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>

<div class="codehilite"><pre><span></span>0.005641937255859375 -0.025215625762939453
</pre></div>


<h2 id="226">2.2.6 正则化<a class="headerlink" href="#226" title="Permanent link">&para;</a></h2>
<p>利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度,这里我们简单的介绍一下正则化的概念</p>
<h3 id="l1">L1正则化<a class="headerlink" href="#l1" title="Permanent link">&para;</a></h3>
<p>损失函数基础上加上权重参数的绝对值</p>
<div>
<div class="MathJax_Preview"> L=E_{in}+\lambda{\sum_j} \left|w_j\right| </div>
<script type="math/tex; mode=display"> L=E_{in}+\lambda{\sum_j} \left|w_j\right| </script>
</div>
<h3 id="l2">L2正则化<a class="headerlink" href="#l2" title="Permanent link">&para;</a></h3>
<p>损失函数基础上加上权重参数的平方和</p>
<div>
<div class="MathJax_Preview"> L=E_{in}+\lambda{\sum_j} w^2_j </div>
<script type="math/tex; mode=display"> L=E_{in}+\lambda{\sum_j} w^2_j </script>
</div>
<p>需要说明的是：l1 相比于 l2 会更容易获得稀疏解，可查看<a href="https://www.zhihu.com/question/37096933/answer/70507353">知乎</a>解答。</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../2_1_4_pytorch-basics-data-loader/" title="2.1.4 Data Loader" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                2.1.4 Data Loader
              </span>
            </div>
          </a>
        
        
          <a href="../2_3_deep-learning-neural-network-introduction/" title="2.3 Deep Learning Neural Network Introduction" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                2.3 Deep Learning Neural Network Introduction
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 <a href="https://github.com/becauseofAI">becauseofAI</a>, Maintained by the <a href="https://github.com/becauseofAI">becauseofAI</a>.
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/becauseofAI" class="md-footer-social__link fa fa-github"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.245445c6.js"></script>
      
        
        
          
          <script src="../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../../assets/javascripts/lunr/lunr.ja.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="../../../js/extra.js"></script>
      
        <script src="../../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>