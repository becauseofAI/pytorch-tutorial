{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Tutorial \u00b6 Introduction \u00b6 PyTorch Tutorial for Deep Learning Research and Product. Note It is still at a very early stage and is being written gradually.","title":"Introduction"},{"location":"#pytorch-tutorial","text":"","title":" PyTorch Tutorial "},{"location":"#introduction","text":"PyTorch Tutorial for Deep Learning Research and Product. Note It is still at a very early stage and is being written gradually.","title":"Introduction"},{"location":"about/","text":"Author \u00b6 becauseofAI Contact \u00b6 Email: helloai777@gmail.com License \u00b6 Apache-2.0","title":"About"},{"location":"about/#author","text":"becauseofAI","title":"Author"},{"location":"about/#contact","text":"Email: helloai777@gmail.com","title":"Contact"},{"location":"about/#license","text":"Apache-2.0","title":"License"},{"location":"tutorial/chapter01_getting-started/","text":"PyTorch \u4e2d\u6587\u624b\u518c\u7b2c\u4e00\u7ae0 \uff1a PyTorch\u5165\u95e8 \u00b6 \u76ee\u5f55 \u00b6 PyTorch \u7b80\u4ecb PyTorch \u73af\u5883\u642d\u5efa PyTorch \u6df1\u5ea6\u5b66\u4e60\uff1a60\u5206\u949f\u5feb\u901f\u5165\u95e8\uff08\u5b98\u65b9\uff09 \u5f20\u91cf Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc \u795e\u7ecf\u7f51\u7edc \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 \u9009\u8bfb\uff1a\u6570\u636e\u5e76\u884c\u5904\u7406\uff08\u591aGPU\uff09 \u76f8\u5173\u8d44\u6e90\u4ecb\u7ecd","title":"PyTorch \u4e2d\u6587\u624b\u518c\u7b2c\u4e00\u7ae0 \uff1a PyTorch\u5165\u95e8"},{"location":"tutorial/chapter01_getting-started/#pytorch-pytorch","text":"","title":"PyTorch \u4e2d\u6587\u624b\u518c\u7b2c\u4e00\u7ae0 \uff1a PyTorch\u5165\u95e8"},{"location":"tutorial/chapter01_getting-started/#_1","text":"PyTorch \u7b80\u4ecb PyTorch \u73af\u5883\u642d\u5efa PyTorch \u6df1\u5ea6\u5b66\u4e60\uff1a60\u5206\u949f\u5feb\u901f\u5165\u95e8\uff08\u5b98\u65b9\uff09 \u5f20\u91cf Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc \u795e\u7ecf\u7f51\u7edc \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 \u9009\u8bfb\uff1a\u6570\u636e\u5e76\u884c\u5904\u7406\uff08\u591aGPU\uff09 \u76f8\u5173\u8d44\u6e90\u4ecb\u7ecd","title":"\u76ee\u5f55"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/","text":"Pytorch \u7b80\u4ecb \u00b6 PyTorch\u7684\u7531\u6765 \u00b6 \u5f88\u591a\u4eba\u90fd\u4f1a\u62ffPyTorch\u548cGoogle\u7684Tensorflow\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fd9\u4e2a\u80af\u5b9a\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u4ed6\u4eec\u662f\u6700\u706b\u7684\u4e24\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e86\u3002\u4f46\u662f\u8bf4\u5230PyTorch\uff0c\u5176\u5b9e\u5e94\u8be5\u5148\u8bf4 Torch \u3002 Torch\u662f\u4ec0\u4e48\uff1f \u00b6 Torch\u82f1\u8bd1\u4e2d\uff1a\u706b\u70ac A Tensor library like Numpy, unlike Numpy it has strong GPU support. Lua is a wrapper for Torch (Yes! you need to have a good understanding of Lua), and for that you will need LuaRocks package manager. Torch\u662f\u4e00\u4e2a\u4e0eNumpy\u7c7b\u4f3c\u7684\u5f20\u91cf\uff08Tensor\uff09\u64cd\u4f5c\u5e93\uff0c\u4e0eNumpy\u4e0d\u540c\u7684\u662fTorch\u5bf9GPU\u652f\u6301\u7684\u5f88\u597d\uff0cLua\u662fTorch\u3002 [1] Torch is not going anywhere. PyTorch and Torch use the same C libraries that contain all the performance: TH, THC, THNN, THCUNN and they will continue to be shared. We still and will have continued engineering on Torch itself, and we have no immediate plan to remove that. PyTorch\u548cTorch\u4f7f\u7528\u5305\u542b\u6240\u6709\u76f8\u540c\u6027\u80fd\u7684C\u5e93\uff1aTH, THC, THNN, THCUNN\uff0c\u5e76\u4e14\u5b83\u4eec\u5c06\u7ee7\u7eed\u5171\u4eab\u8fd9\u4e9b\u5e93\u3002 \u8fd9\u6837\u7684\u56de\u7b54\u5c31\u5f88\u660e\u786e\u4e86\uff0c\u5176\u5b9ePyTorch\u548cTorch\u90fd\u4f7f\u7528\u7684\u662f\u76f8\u540c\u7684\u5e95\u5c42\uff0c\u53ea\u662f\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u4e0a\u5c42\u5305\u88c5\u8bed\u8a00\u3002 LUA\u867d\u7136\u5feb\uff0c\u4f46\u662f\u592a\u5c0f\u4f17\u4e86\uff0c\u6240\u4ee5\u624d\u4f1a\u6709PyTorch\u7684\u51fa\u73b0\u3002 [2] \u91cd\u65b0\u4ecb\u7ecd PyTorch \u00b6 PyTorch is an open source machine learning library for Python, based on Torch, used for applications such as natural language processing. It is primarily developed by Facebook's artificial-intelligence research group, and Uber's \"Pyro\" software for probabilistic programming is built on it. PyTorch\u662f\u4e00\u4e2a\u57fa\u4e8eTorch\u7684Python\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u5e93\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002 \u5b83\u4e3b\u8981\u7531Facebook\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5c0f\u7ec4\u5f00\u53d1\u3002Uber\u7684\"Pyro\"\u4e5f\u662f\u4f7f\u7528\u7684\u8fd9\u4e2a\u5e93\u3002 [3] PyTorch is a Python package that provides two high-level features: - Tensor computation (like NumPy) with strong GPU acceleration - Deep neural networks built on a tape-based autograd system You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed. PyTorch\u662f\u4e00\u4e2aPython\u5305\uff0c\u63d0\u4f9b\u4e24\u4e2a\u9ad8\u7ea7\u529f\u80fd\uff1a * \u5177\u6709\u5f3a\u5927\u7684GPU\u52a0\u901f\u7684\u5f20\u91cf\u8ba1\u7b97\uff08\u5982NumPy\uff09 * \u5305\u542b\u81ea\u52a8\u6c42\u5bfc\u7cfb\u7edf\u7684\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc \u4efb\u4f55\u65f6\u5019\uff0c\u4f60\u53ef\u4ee5\u7528\u4f60\u559c\u6b22\u7684Python\u5305\uff0c\u5982NumPy\u3001SciPy \u548c Cython\u53bb\u6269\u5c55PyTorch\u3002 [4] \u5bf9\u6bd4PyTorch\u548cTensorflow \u00b6 \u6ca1\u6709\u597d\u7684\u6846\u67b6\uff0c\u53ea\u6709\u5408\u9002\u7684\u6846\u67b6\uff0c \u8fd9\u7bc7\u77e5\u4e4e\u6587\u7ae0 \u6709\u4e2a\u7b80\u5355\u7684\u5bf9\u6bd4\uff0c2019\u5e746\u670825\u65e5[\u673a\u5668\u4e4b\u5fc3]\u7ffb\u8bd1\u7684 PyTorch\u548cKeras\u7684\u6700\u65b0\u5bf9\u6bd4 \uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u4e0d\u8be6\u7ec6\u518d\u8bf4\u4e86\u3002 \u5e76\u4e14\u6280\u672f\u662f\u53d1\u5c55\u7684\uff0c\u77e5\u4e4e\u4e0a\u7684\u5bf9\u6bd4\u4e5f\u4e0d\u662f\u7edd\u5bf9\u7684\uff0c\u6bd4\u5982Tensorflow\u57281.5\u7248\u7684\u65f6\u5019\u5c31\u5f15\u5165\u4e86Eager Execution\u673a\u5236\u5b9e\u73b0\u4e86\u52a8\u6001\u56fe\uff0cPyTorch\u7684\u53ef\u89c6\u5316,windows\u652f\u6301\uff0c\u6cbf\u7ef4\u7ffb\u8f6c\u5f20\u91cf\u7b49\u95ee\u9898\u90fd\u5df2\u7ecf\u4e0d\u662f\u95ee\u9898\u4e86\u3002 \u518d\u6b21\u603b\u7ed3 \u00b6 PyTorch\u7b97\u662f\u76f8\u5f53\u7b80\u6d01\u4f18\u96c5\u4e14\u9ad8\u6548\u5feb\u901f\u7684\u6846\u67b6 \u8bbe\u8ba1\u8ffd\u6c42\u6700\u5c11\u7684\u5c01\u88c5\uff0c\u5c3d\u91cf\u907f\u514d\u91cd\u590d\u9020\u8f6e\u5b50 \u7b97\u662f\u6240\u6709\u7684\u6846\u67b6\u4e2d\u9762\u5411\u5bf9\u8c61\u8bbe\u8ba1\u7684\u6700\u4f18\u96c5\u7684\u4e00\u4e2a\uff0c\u8bbe\u8ba1\u6700\u7b26\u5408\u4eba\u4eec\u7684\u601d\u7ef4\uff0c\u5b83\u8ba9\u7528\u6237\u5c3d\u53ef\u80fd\u5730\u4e13\u6ce8\u4e8e\u5b9e\u73b0\u81ea\u5df1\u7684\u60f3\u6cd5 \u5927\u4f6c\u652f\u6301\uff0c\u4e0egoogle\u7684Tensorflow\u7c7b\u4f3c\uff0cFAIR\u7684\u652f\u6301\u8db3\u4ee5\u786e\u4fddPyTorch\u83b7\u5f97\u6301\u7eed\u7684\u5f00\u53d1\u66f4\u65b0 \u4e0d\u9519\u7684\u7684\u6587\u6863\uff08\u76f8\u6bd4FB\u7684\u5176\u4ed6\u9879\u76ee\uff0cPyTorch\u7684\u6587\u6863\u7b80\u76f4\u7b97\u662f\u5b8c\u5584\u4e86\uff0c\u53c2\u8003Thrift\uff09\uff0cPyTorch\u4f5c\u8005\u4eb2\u81ea\u7ef4\u62a4\u7684\u8bba\u575b \u4f9b\u7528\u6237\u4ea4\u6d41\u548c\u6c42\u6559\u95ee\u9898 \u5165\u95e8\u7b80\u5355","title":"1.1 PyTorch Tntroduction"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/#pytorch","text":"","title":"Pytorch \u7b80\u4ecb"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/#pytorch_1","text":"\u5f88\u591a\u4eba\u90fd\u4f1a\u62ffPyTorch\u548cGoogle\u7684Tensorflow\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fd9\u4e2a\u80af\u5b9a\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u4ed6\u4eec\u662f\u6700\u706b\u7684\u4e24\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e86\u3002\u4f46\u662f\u8bf4\u5230PyTorch\uff0c\u5176\u5b9e\u5e94\u8be5\u5148\u8bf4 Torch \u3002","title":"PyTorch\u7684\u7531\u6765"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/#torch","text":"Torch\u82f1\u8bd1\u4e2d\uff1a\u706b\u70ac A Tensor library like Numpy, unlike Numpy it has strong GPU support. Lua is a wrapper for Torch (Yes! you need to have a good understanding of Lua), and for that you will need LuaRocks package manager. Torch\u662f\u4e00\u4e2a\u4e0eNumpy\u7c7b\u4f3c\u7684\u5f20\u91cf\uff08Tensor\uff09\u64cd\u4f5c\u5e93\uff0c\u4e0eNumpy\u4e0d\u540c\u7684\u662fTorch\u5bf9GPU\u652f\u6301\u7684\u5f88\u597d\uff0cLua\u662fTorch\u3002 [1] Torch is not going anywhere. PyTorch and Torch use the same C libraries that contain all the performance: TH, THC, THNN, THCUNN and they will continue to be shared. We still and will have continued engineering on Torch itself, and we have no immediate plan to remove that. PyTorch\u548cTorch\u4f7f\u7528\u5305\u542b\u6240\u6709\u76f8\u540c\u6027\u80fd\u7684C\u5e93\uff1aTH, THC, THNN, THCUNN\uff0c\u5e76\u4e14\u5b83\u4eec\u5c06\u7ee7\u7eed\u5171\u4eab\u8fd9\u4e9b\u5e93\u3002 \u8fd9\u6837\u7684\u56de\u7b54\u5c31\u5f88\u660e\u786e\u4e86\uff0c\u5176\u5b9ePyTorch\u548cTorch\u90fd\u4f7f\u7528\u7684\u662f\u76f8\u540c\u7684\u5e95\u5c42\uff0c\u53ea\u662f\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u4e0a\u5c42\u5305\u88c5\u8bed\u8a00\u3002 LUA\u867d\u7136\u5feb\uff0c\u4f46\u662f\u592a\u5c0f\u4f17\u4e86\uff0c\u6240\u4ee5\u624d\u4f1a\u6709PyTorch\u7684\u51fa\u73b0\u3002 [2]","title":"Torch\u662f\u4ec0\u4e48\uff1f"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/#pytorch_2","text":"PyTorch is an open source machine learning library for Python, based on Torch, used for applications such as natural language processing. It is primarily developed by Facebook's artificial-intelligence research group, and Uber's \"Pyro\" software for probabilistic programming is built on it. PyTorch\u662f\u4e00\u4e2a\u57fa\u4e8eTorch\u7684Python\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u5e93\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002 \u5b83\u4e3b\u8981\u7531Facebook\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5c0f\u7ec4\u5f00\u53d1\u3002Uber\u7684\"Pyro\"\u4e5f\u662f\u4f7f\u7528\u7684\u8fd9\u4e2a\u5e93\u3002 [3] PyTorch is a Python package that provides two high-level features: - Tensor computation (like NumPy) with strong GPU acceleration - Deep neural networks built on a tape-based autograd system You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed. PyTorch\u662f\u4e00\u4e2aPython\u5305\uff0c\u63d0\u4f9b\u4e24\u4e2a\u9ad8\u7ea7\u529f\u80fd\uff1a * \u5177\u6709\u5f3a\u5927\u7684GPU\u52a0\u901f\u7684\u5f20\u91cf\u8ba1\u7b97\uff08\u5982NumPy\uff09 * \u5305\u542b\u81ea\u52a8\u6c42\u5bfc\u7cfb\u7edf\u7684\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc \u4efb\u4f55\u65f6\u5019\uff0c\u4f60\u53ef\u4ee5\u7528\u4f60\u559c\u6b22\u7684Python\u5305\uff0c\u5982NumPy\u3001SciPy \u548c Cython\u53bb\u6269\u5c55PyTorch\u3002 [4]","title":"\u91cd\u65b0\u4ecb\u7ecd PyTorch"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/#pytorchtensorflow","text":"\u6ca1\u6709\u597d\u7684\u6846\u67b6\uff0c\u53ea\u6709\u5408\u9002\u7684\u6846\u67b6\uff0c \u8fd9\u7bc7\u77e5\u4e4e\u6587\u7ae0 \u6709\u4e2a\u7b80\u5355\u7684\u5bf9\u6bd4\uff0c2019\u5e746\u670825\u65e5[\u673a\u5668\u4e4b\u5fc3]\u7ffb\u8bd1\u7684 PyTorch\u548cKeras\u7684\u6700\u65b0\u5bf9\u6bd4 \uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u4e0d\u8be6\u7ec6\u518d\u8bf4\u4e86\u3002 \u5e76\u4e14\u6280\u672f\u662f\u53d1\u5c55\u7684\uff0c\u77e5\u4e4e\u4e0a\u7684\u5bf9\u6bd4\u4e5f\u4e0d\u662f\u7edd\u5bf9\u7684\uff0c\u6bd4\u5982Tensorflow\u57281.5\u7248\u7684\u65f6\u5019\u5c31\u5f15\u5165\u4e86Eager Execution\u673a\u5236\u5b9e\u73b0\u4e86\u52a8\u6001\u56fe\uff0cPyTorch\u7684\u53ef\u89c6\u5316,windows\u652f\u6301\uff0c\u6cbf\u7ef4\u7ffb\u8f6c\u5f20\u91cf\u7b49\u95ee\u9898\u90fd\u5df2\u7ecf\u4e0d\u662f\u95ee\u9898\u4e86\u3002","title":"\u5bf9\u6bd4PyTorch\u548cTensorflow"},{"location":"tutorial/chapter01_getting-started/1_1_pytorch-introduction/#_1","text":"PyTorch\u7b97\u662f\u76f8\u5f53\u7b80\u6d01\u4f18\u96c5\u4e14\u9ad8\u6548\u5feb\u901f\u7684\u6846\u67b6 \u8bbe\u8ba1\u8ffd\u6c42\u6700\u5c11\u7684\u5c01\u88c5\uff0c\u5c3d\u91cf\u907f\u514d\u91cd\u590d\u9020\u8f6e\u5b50 \u7b97\u662f\u6240\u6709\u7684\u6846\u67b6\u4e2d\u9762\u5411\u5bf9\u8c61\u8bbe\u8ba1\u7684\u6700\u4f18\u96c5\u7684\u4e00\u4e2a\uff0c\u8bbe\u8ba1\u6700\u7b26\u5408\u4eba\u4eec\u7684\u601d\u7ef4\uff0c\u5b83\u8ba9\u7528\u6237\u5c3d\u53ef\u80fd\u5730\u4e13\u6ce8\u4e8e\u5b9e\u73b0\u81ea\u5df1\u7684\u60f3\u6cd5 \u5927\u4f6c\u652f\u6301\uff0c\u4e0egoogle\u7684Tensorflow\u7c7b\u4f3c\uff0cFAIR\u7684\u652f\u6301\u8db3\u4ee5\u786e\u4fddPyTorch\u83b7\u5f97\u6301\u7eed\u7684\u5f00\u53d1\u66f4\u65b0 \u4e0d\u9519\u7684\u7684\u6587\u6863\uff08\u76f8\u6bd4FB\u7684\u5176\u4ed6\u9879\u76ee\uff0cPyTorch\u7684\u6587\u6863\u7b80\u76f4\u7b97\u662f\u5b8c\u5584\u4e86\uff0c\u53c2\u8003Thrift\uff09\uff0cPyTorch\u4f5c\u8005\u4eb2\u81ea\u7ef4\u62a4\u7684\u8bba\u575b \u4f9b\u7528\u6237\u4ea4\u6d41\u548c\u6c42\u6559\u95ee\u9898 \u5165\u95e8\u7b80\u5355","title":"\u518d\u6b21\u603b\u7ed3"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/","text":"PyTorch\u662f\u4ec0\u4e48? \u00b6 \u57fa\u4e8ePython\u7684\u79d1\u5b66\u8ba1\u7b97\u5305\uff0c\u670d\u52a1\u4e8e\u4ee5\u4e0b\u4e24\u79cd\u573a\u666f: \u4f5c\u4e3aNumPy\u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u4f7f\u7528GPU\u7684\u5f3a\u5927\u8ba1\u7b97\u80fd\u529b \u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u9ad8\u901f\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0 \u5f00\u59cb \u00b6 Tensors\uff08\u5f20\u91cf\uff09 \u00b6 Tensors\u4e0eNumpy\u4e2d\u7684 ndarrays\u7c7b\u4f3c\uff0c\u4f46\u662f\u5728PyTorch\u4e2d Tensors \u53ef\u4ee5\u4f7f\u7528GPU\u8fdb\u884c\u8ba1\u7b97. from __future__ import print_function import torch \u521b\u5efa\u4e00\u4e2a 5x3 \u77e9\u9635, \u4f46\u662f\u672a\u521d\u59cb\u5316: x = torch . empty ( 5 , 3 ) print ( x ) tensor([[0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000]]) \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch . rand ( 5 , 3 ) print ( x ) tensor([[0.6972, 0.0231, 0.3087], [0.2083, 0.6141, 0.6896], [0.7228, 0.9715, 0.5304], [0.7727, 0.1621, 0.9777], [0.6526, 0.6170, 0.2605]]) \u521b\u5efa\u4e00\u4e2a0\u586b\u5145\u7684\u77e9\u9635\uff0c\u6570\u636e\u7c7b\u578b\u4e3along: x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u521b\u5efatensor\u5e76\u4f7f\u7528\u73b0\u6709\u6570\u636e\u521d\u59cb\u5316: x = torch . tensor ([ 5.5 , 3 ]) print ( x ) tensor([5.5000, 3.0000]) \u6839\u636e\u73b0\u6709\u7684\u5f20\u91cf\u521b\u5efa\u5f20\u91cf\u3002 \u8fd9\u4e9b\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165\u5f20\u91cf\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\uff0c dtype\uff0c\u9664\u975e\u8bbe\u7f6e\u65b0\u7684\u503c\u8fdb\u884c\u8986\u76d6 x = x . new_ones ( 5 , 3 , dtype = torch . double ) # new_* \u65b9\u6cd5\u6765\u521b\u5efa\u5bf9\u8c61 print ( x ) x = torch . randn_like ( x , dtype = torch . float ) # \u8986\u76d6 dtype! print ( x ) # \u5bf9\u8c61\u7684size \u662f\u76f8\u540c\u7684\uff0c\u53ea\u662f\u503c\u548c\u7c7b\u578b\u53d1\u751f\u4e86\u53d8\u5316 tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.5691, -2.0126, -0.4064], [-0.0863, 0.4692, -1.1209], [-1.1177, -0.5764, -0.5363], [-0.4390, 0.6688, 0.0889], [ 1.3334, -1.1600, 1.8457]]) \u83b7\u53d6 size \u8bd1\u8005\u6ce8\uff1a\u4f7f\u7528size\u65b9\u6cd5\u4e0eNumpy\u7684shape\u5c5e\u6027\u8fd4\u56de\u7684\u76f8\u540c\uff0c\u5f20\u91cf\u4e5f\u652f\u6301shape\u5c5e\u6027\uff0c\u540e\u9762\u4f1a\u8be6\u7ec6\u4ecb\u7ecd print ( x . size ()) torch.Size([5, 3]) Note torch.Size \u8fd4\u56de\u503c\u662f tuple\u7c7b\u578b, \u6240\u4ee5\u5b83\u652f\u6301tuple\u7c7b\u578b\u7684\u6240\u6709\u64cd\u4f5c\u3002 Operations\uff08\u64cd\u4f5c\uff09 \u00b6 \u64cd\u4f5c\u6709\u591a\u79cd\u8bed\u6cd5\u3002 \u6211\u4eec\u5c06\u770b\u4e00\u4e0b\u52a0\u6cd5\u8fd0\u7b97\u3002 \u52a0\u6cd5\uff1a\u8bed\u6cd51 y = torch . rand ( 5 , 3 ) print ( x + y ) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) \u52a0\u6cd5\uff1a\u8bed\u6cd52 print ( torch . add ( x , y )) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) \u52a0\u6cd5\uff1a\u63d0\u4f9b\u8f93\u51fatensor\u4f5c\u4e3a\u53c2\u6570 result = torch . empty ( 5 , 3 ) torch . add ( x , y , out = result ) print ( result ) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) \u52a0\u6cd5\uff1a\u539f\u5730 # adds x to y y . add_ ( x ) print ( y ) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) Note \u4efb\u4f55 \u4ee5 _ \u7ed3\u5c3e\u7684\u64cd\u4f5c\u90fd\u4f1a\u7528\u7ed3\u679c\u66ff\u6362\u539f\u53d8\u91cf. \u4f8b\u5982: x.copy_(y) , x.t_() , \u90fd\u4f1a\u6539\u53d8 x \u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528\u4e0eNumPy\u7d22\u5f15\u65b9\u5f0f\u76f8\u540c\u7684\u64cd\u4f5c\u6765\u8fdb\u884c\u5bf9\u5f20\u91cf\u7684\u64cd\u4f5c! print ( x [:, 1 ]) tensor([-2.0126, 0.4692, -0.5764, 0.6688, -1.1600]) \u6539\u53d8\u5927\u5c0f\uff1a\u53ef\u4ee5\u7528 torch.view \u6539\u53d8\u5f20\u91cf\u7684\u7ef4\u5ea6\u548c\u5927\u5c0f \u8bd1\u8005\u6ce8\uff1atorch.view \u4e0eNumpy\u7684reshape\u7c7b\u4f3c x = torch . randn ( 4 , 4 ) y = x . view ( 16 ) z = x . view ( - 1 , 8 ) # size -1 \u4ece\u5176\u4ed6\u7ef4\u5ea6\u63a8\u65ad print ( x . size (), y . size (), z . size ()) torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5982\u679c\u4f60\u6709\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u5f20\u91cf\uff0c\u4f7f\u7528 .item() \u6765\u5f97\u5230Python\u6570\u636e\u7c7b\u578b\u7684\u6570\u503c x = torch . randn ( 1 ) print ( x ) print ( x . item ()) tensor([-0.2368]) -0.23680149018764496 Read later: 100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc., are described here <https://pytorch.org/docs/torch> _. NumPy \u8f6c\u6362 \u00b6 Converting a Torch Tensor to a NumPy array and vice versa is a breeze. The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other. Torch Tensor \u8f6c\u6210 NumPy Array \u00b6 a = torch . ones ( 5 ) print ( a ) tensor([1., 1., 1., 1., 1.]) b = a . numpy () print ( b ) [1. 1. 1. 1. 1.] See how the numpy array changed in value. a . add_ ( 1 ) print ( a ) print ( b ) tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] NumPy Array \u8f6c\u6210 Torch Tensor \u00b6 \u4f7f\u7528from_numpy\u81ea\u52a8\u8f6c\u5316 import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) \u6240\u6709\u7684 Tensor \u7c7b\u578b\u9ed8\u8ba4\u90fd\u662f\u57fa\u4e8eCPU\uff0c CharTensor \u7c7b\u578b\u4e0d\u652f\u6301\u5230 NumPy \u7684\u8f6c\u6362. CUDA \u5f20\u91cf \u00b6 \u4f7f\u7528 .to \u65b9\u6cd5 \u53ef\u4ee5\u5c06Tensor\u79fb\u52a8\u5230\u4efb\u4f55\u8bbe\u5907\u4e2d # is_available \u51fd\u6570\u5224\u65ad\u662f\u5426\u6709cuda\u53ef\u4ee5\u4f7f\u7528 # ``torch.device``\u5c06\u5f20\u91cf\u79fb\u52a8\u5230\u6307\u5b9a\u7684\u8bbe\u5907\u4e2d if torch . cuda . is_available (): device = torch . device ( \"cuda\" ) # a CUDA \u8bbe\u5907\u5bf9\u8c61 y = torch . ones_like ( x , device = device ) # \u76f4\u63a5\u4eceGPU\u521b\u5efa\u5f20\u91cf x = x . to ( device ) # \u6216\u8005\u76f4\u63a5\u4f7f\u7528``.to(\"cuda\")``\u5c06\u5f20\u91cf\u79fb\u52a8\u5230cuda\u4e2d z = x + y print ( z ) print ( z . to ( \"cpu\" , torch . double )) # ``.to`` \u4e5f\u4f1a\u5bf9\u53d8\u91cf\u7684\u7c7b\u578b\u505a\u66f4\u6539 tensor([0.7632], device='cuda:0') tensor([0.7632], dtype=torch.float64)","title":"1.3.1 Tensor"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#pytorch","text":"\u57fa\u4e8ePython\u7684\u79d1\u5b66\u8ba1\u7b97\u5305\uff0c\u670d\u52a1\u4e8e\u4ee5\u4e0b\u4e24\u79cd\u573a\u666f: \u4f5c\u4e3aNumPy\u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u4f7f\u7528GPU\u7684\u5f3a\u5927\u8ba1\u7b97\u80fd\u529b \u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u9ad8\u901f\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0","title":"PyTorch\u662f\u4ec0\u4e48?"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#_1","text":"","title":"\u5f00\u59cb"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#tensors","text":"Tensors\u4e0eNumpy\u4e2d\u7684 ndarrays\u7c7b\u4f3c\uff0c\u4f46\u662f\u5728PyTorch\u4e2d Tensors \u53ef\u4ee5\u4f7f\u7528GPU\u8fdb\u884c\u8ba1\u7b97. from __future__ import print_function import torch \u521b\u5efa\u4e00\u4e2a 5x3 \u77e9\u9635, \u4f46\u662f\u672a\u521d\u59cb\u5316: x = torch . empty ( 5 , 3 ) print ( x ) tensor([[0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000]]) \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch . rand ( 5 , 3 ) print ( x ) tensor([[0.6972, 0.0231, 0.3087], [0.2083, 0.6141, 0.6896], [0.7228, 0.9715, 0.5304], [0.7727, 0.1621, 0.9777], [0.6526, 0.6170, 0.2605]]) \u521b\u5efa\u4e00\u4e2a0\u586b\u5145\u7684\u77e9\u9635\uff0c\u6570\u636e\u7c7b\u578b\u4e3along: x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u521b\u5efatensor\u5e76\u4f7f\u7528\u73b0\u6709\u6570\u636e\u521d\u59cb\u5316: x = torch . tensor ([ 5.5 , 3 ]) print ( x ) tensor([5.5000, 3.0000]) \u6839\u636e\u73b0\u6709\u7684\u5f20\u91cf\u521b\u5efa\u5f20\u91cf\u3002 \u8fd9\u4e9b\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165\u5f20\u91cf\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\uff0c dtype\uff0c\u9664\u975e\u8bbe\u7f6e\u65b0\u7684\u503c\u8fdb\u884c\u8986\u76d6 x = x . new_ones ( 5 , 3 , dtype = torch . double ) # new_* \u65b9\u6cd5\u6765\u521b\u5efa\u5bf9\u8c61 print ( x ) x = torch . randn_like ( x , dtype = torch . float ) # \u8986\u76d6 dtype! print ( x ) # \u5bf9\u8c61\u7684size \u662f\u76f8\u540c\u7684\uff0c\u53ea\u662f\u503c\u548c\u7c7b\u578b\u53d1\u751f\u4e86\u53d8\u5316 tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.5691, -2.0126, -0.4064], [-0.0863, 0.4692, -1.1209], [-1.1177, -0.5764, -0.5363], [-0.4390, 0.6688, 0.0889], [ 1.3334, -1.1600, 1.8457]]) \u83b7\u53d6 size \u8bd1\u8005\u6ce8\uff1a\u4f7f\u7528size\u65b9\u6cd5\u4e0eNumpy\u7684shape\u5c5e\u6027\u8fd4\u56de\u7684\u76f8\u540c\uff0c\u5f20\u91cf\u4e5f\u652f\u6301shape\u5c5e\u6027\uff0c\u540e\u9762\u4f1a\u8be6\u7ec6\u4ecb\u7ecd print ( x . size ()) torch.Size([5, 3]) Note torch.Size \u8fd4\u56de\u503c\u662f tuple\u7c7b\u578b, \u6240\u4ee5\u5b83\u652f\u6301tuple\u7c7b\u578b\u7684\u6240\u6709\u64cd\u4f5c\u3002","title":"Tensors\uff08\u5f20\u91cf\uff09"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#operations","text":"\u64cd\u4f5c\u6709\u591a\u79cd\u8bed\u6cd5\u3002 \u6211\u4eec\u5c06\u770b\u4e00\u4e0b\u52a0\u6cd5\u8fd0\u7b97\u3002 \u52a0\u6cd5\uff1a\u8bed\u6cd51 y = torch . rand ( 5 , 3 ) print ( x + y ) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) \u52a0\u6cd5\uff1a\u8bed\u6cd52 print ( torch . add ( x , y )) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) \u52a0\u6cd5\uff1a\u63d0\u4f9b\u8f93\u51fatensor\u4f5c\u4e3a\u53c2\u6570 result = torch . empty ( 5 , 3 ) torch . add ( x , y , out = result ) print ( result ) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) \u52a0\u6cd5\uff1a\u539f\u5730 # adds x to y y . add_ ( x ) print ( y ) tensor([[ 0.7808, -1.4388, 0.3151], [-0.0076, 1.0716, -0.8465], [-0.8175, 0.3625, -0.2005], [ 0.2435, 0.8512, 0.7142], [ 1.4737, -0.8545, 2.4833]]) Note \u4efb\u4f55 \u4ee5 _ \u7ed3\u5c3e\u7684\u64cd\u4f5c\u90fd\u4f1a\u7528\u7ed3\u679c\u66ff\u6362\u539f\u53d8\u91cf. \u4f8b\u5982: x.copy_(y) , x.t_() , \u90fd\u4f1a\u6539\u53d8 x \u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528\u4e0eNumPy\u7d22\u5f15\u65b9\u5f0f\u76f8\u540c\u7684\u64cd\u4f5c\u6765\u8fdb\u884c\u5bf9\u5f20\u91cf\u7684\u64cd\u4f5c! print ( x [:, 1 ]) tensor([-2.0126, 0.4692, -0.5764, 0.6688, -1.1600]) \u6539\u53d8\u5927\u5c0f\uff1a\u53ef\u4ee5\u7528 torch.view \u6539\u53d8\u5f20\u91cf\u7684\u7ef4\u5ea6\u548c\u5927\u5c0f \u8bd1\u8005\u6ce8\uff1atorch.view \u4e0eNumpy\u7684reshape\u7c7b\u4f3c x = torch . randn ( 4 , 4 ) y = x . view ( 16 ) z = x . view ( - 1 , 8 ) # size -1 \u4ece\u5176\u4ed6\u7ef4\u5ea6\u63a8\u65ad print ( x . size (), y . size (), z . size ()) torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5982\u679c\u4f60\u6709\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u5f20\u91cf\uff0c\u4f7f\u7528 .item() \u6765\u5f97\u5230Python\u6570\u636e\u7c7b\u578b\u7684\u6570\u503c x = torch . randn ( 1 ) print ( x ) print ( x . item ()) tensor([-0.2368]) -0.23680149018764496 Read later: 100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc., are described here <https://pytorch.org/docs/torch> _.","title":"Operations\uff08\u64cd\u4f5c\uff09"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#numpy","text":"Converting a Torch Tensor to a NumPy array and vice versa is a breeze. The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other.","title":"NumPy \u8f6c\u6362"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#torch-tensor-numpy-array","text":"a = torch . ones ( 5 ) print ( a ) tensor([1., 1., 1., 1., 1.]) b = a . numpy () print ( b ) [1. 1. 1. 1. 1.] See how the numpy array changed in value. a . add_ ( 1 ) print ( a ) print ( b ) tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]","title":"Torch Tensor \u8f6c\u6210 NumPy Array"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#numpy-array-torch-tensor","text":"\u4f7f\u7528from_numpy\u81ea\u52a8\u8f6c\u5316 import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) \u6240\u6709\u7684 Tensor \u7c7b\u578b\u9ed8\u8ba4\u90fd\u662f\u57fa\u4e8eCPU\uff0c CharTensor \u7c7b\u578b\u4e0d\u652f\u6301\u5230 NumPy \u7684\u8f6c\u6362.","title":"NumPy Array \u8f6c\u6210 Torch Tensor"},{"location":"tutorial/chapter01_getting-started/1_3_1_tensor_tutorial/#cuda","text":"\u4f7f\u7528 .to \u65b9\u6cd5 \u53ef\u4ee5\u5c06Tensor\u79fb\u52a8\u5230\u4efb\u4f55\u8bbe\u5907\u4e2d # is_available \u51fd\u6570\u5224\u65ad\u662f\u5426\u6709cuda\u53ef\u4ee5\u4f7f\u7528 # ``torch.device``\u5c06\u5f20\u91cf\u79fb\u52a8\u5230\u6307\u5b9a\u7684\u8bbe\u5907\u4e2d if torch . cuda . is_available (): device = torch . device ( \"cuda\" ) # a CUDA \u8bbe\u5907\u5bf9\u8c61 y = torch . ones_like ( x , device = device ) # \u76f4\u63a5\u4eceGPU\u521b\u5efa\u5f20\u91cf x = x . to ( device ) # \u6216\u8005\u76f4\u63a5\u4f7f\u7528``.to(\"cuda\")``\u5c06\u5f20\u91cf\u79fb\u52a8\u5230cuda\u4e2d z = x + y print ( z ) print ( z . to ( \"cpu\" , torch . double )) # ``.to`` \u4e5f\u4f1a\u5bf9\u53d8\u91cf\u7684\u7c7b\u578b\u505a\u66f4\u6539 tensor([0.7632], device='cuda:0') tensor([0.7632], dtype=torch.float64)","title":"CUDA \u5f20\u91cf"},{"location":"tutorial/chapter01_getting-started/1_3_2_autograd_tutorial/","text":"% matplotlib inline Autograd: \u81ea\u52a8\u6c42\u5bfc\u673a\u5236 \u00b6 PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662f autograd \u5305\u3002 \u6211\u4eec\u5148\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b\u8fd9\u4e2a\u5305\uff0c\u7136\u540e\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u3002 autograd \u5305\u4e3a\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u63d0\u4f9b\u4e86\u81ea\u52a8\u6c42\u5bfc\u3002 \u5b83\u662f\u4e00\u4e2a\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad\u662f\u6839\u636e\u4f60\u7684\u4ee3\u7801\u6765\u786e\u5b9a\u5982\u4f55\u8fd0\u884c\uff0c\u5e76\u4e14\u6bcf\u6b21\u8fed\u4ee3\u53ef\u4ee5\u662f\u4e0d\u540c\u7684\u3002 \u793a\u4f8b \u5f20\u91cf\uff08Tensor\uff09 \u00b6 torch.Tensor \u662f\u8fd9\u4e2a\u5305\u7684\u6838\u5fc3\u7c7b\u3002\u5982\u679c\u8bbe\u7f6e .requires_grad \u4e3a True \uff0c\u90a3\u4e48\u5c06\u4f1a\u8ffd\u8e2a\u6240\u6709\u5bf9\u4e8e\u8be5\u5f20\u91cf\u7684\u64cd\u4f5c\u3002 \u5f53\u5b8c\u6210\u8ba1\u7b97\u540e\u901a\u8fc7\u8c03\u7528 .backward() \uff0c\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6\uff0c \u8fd9\u4e2a\u5f20\u91cf\u7684\u6240\u6709\u68af\u5ea6\u5c06\u4f1a\u81ea\u52a8\u79ef\u7d2f\u5230 .grad \u5c5e\u6027\u3002 \u8981\u963b\u6b62\u5f20\u91cf\u8ddf\u8e2a\u5386\u53f2\u8bb0\u5f55\uff0c\u53ef\u4ee5\u8c03\u7528 .detach() \u65b9\u6cd5\u5c06\u5176\u4e0e\u8ba1\u7b97\u5386\u53f2\u8bb0\u5f55\u5206\u79bb\uff0c\u5e76\u7981\u6b62\u8ddf\u8e2a\u5b83\u5c06\u6765\u7684\u8ba1\u7b97\u8bb0\u5f55\u3002 \u4e3a\u4e86\u9632\u6b62\u8ddf\u8e2a\u5386\u53f2\u8bb0\u5f55\uff08\u548c\u4f7f\u7528\u5185\u5b58\uff09\uff0c\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u5305\u88c5\u5728 with torch.no_grad()\uff1a \u4e2d\u3002 \u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u7279\u522b\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709 requires_grad = True \u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4f46\u662f\u6211\u4eec\u4e0d\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u3002 \u5728\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u4e2d\u8fd8\u6709\u53e6\u5916\u4e00\u4e2a\u91cd\u8981\u7684\u7c7b Function . Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor (except for Tensors created by the user - their grad_fn is None ). Tensor \u548c Function \u4e92\u76f8\u8fde\u63a5\u5e76\u751f\u6210\u4e00\u4e2a\u975e\u5faa\u73af\u56fe\uff0c\u5b83\u8868\u793a\u548c\u5b58\u50a8\u4e86\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u3002 \u6bcf\u4e2a\u5f20\u91cf\u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027\uff0c\u8fd9\u4e2a\u5c5e\u6027\u5f15\u7528\u4e86\u4e00\u4e2a\u521b\u5efa\u4e86 Tensor \u7684 Function \uff08\u9664\u975e\u8fd9\u4e2a\u5f20\u91cf\u662f\u7528\u6237\u624b\u52a8\u521b\u5efa\u7684\uff0c\u5373\uff0c\u8fd9\u4e2a\u5f20\u91cf\u7684 grad_fn \u662f None \uff09\u3002 \u5982\u679c\u9700\u8981\u8ba1\u7b97\u5bfc\u6570\uff0c\u4f60\u53ef\u4ee5\u5728 Tensor \u4e0a\u8c03\u7528 .backward() \u3002 \u5982\u679c Tensor \u662f\u4e00\u4e2a\u6807\u91cf\uff08\u5373\u5b83\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u6570\u636e\uff09\u5219\u4e0d\u9700\u8981\u4e3a backward() \u6307\u5b9a\u4efb\u4f55\u53c2\u6570\uff0c \u4f46\u662f\u5982\u679c\u5b83\u6709\u66f4\u591a\u7684\u5143\u7d20\uff0c\u4f60\u9700\u8981\u6307\u5b9a\u4e00\u4e2a gradient \u53c2\u6570\u6765\u5339\u914d\u5f20\u91cf\u7684\u5f62\u72b6\u3002 \u8bd1\u8005\u6ce8\uff1a\u5728\u5176\u4ed6\u7684\u6587\u7ae0\u4e2d\u4f60\u53ef\u80fd\u4f1a\u770b\u5230\u8bf4\u5c06Tensor\u5305\u88f9\u5230Variable\u4e2d\u63d0\u4f9b\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\uff0cVariable \u8fd9\u4e2a\u57280.41\u7248\u4e2d\u5df2\u7ecf\u88ab\u6807\u6ce8\u4e3a\u8fc7\u671f\u4e86\uff0c\u73b0\u5728\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528Tensor\uff0c\u5b98\u65b9\u6587\u6863\u5728\u8fd9\u91cc\uff1a ( https://pytorch.org/docs/stable/autograd.html#variable-deprecated ) \u5177\u4f53\u7684\u540e\u9762\u4f1a\u6709\u8be6\u7ec6\u8bf4\u660e import torch \u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\u5e76\u8bbe\u7f6e requires_grad=True \u7528\u6765\u8ffd\u8e2a\u4ed6\u7684\u8ba1\u7b97\u5386\u53f2 x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) tensor([[1., 1.], [1., 1.]], requires_grad=True) \u5bf9\u5f20\u91cf\u8fdb\u884c\u64cd\u4f5c: y = x + 2 print ( y ) tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward>) \u7ed3\u679c y \u5df2\u7ecf\u88ab\u8ba1\u7b97\u51fa\u6765\u4e86\uff0c\u6240\u4ee5\uff0c grad_fn \u5df2\u7ecf\u88ab\u81ea\u52a8\u751f\u6210\u4e86\u3002 print ( y . grad_fn ) <AddBackward object at 0x00000232535FD860> \u5bf9y\u8fdb\u884c\u4e00\u4e2a\u64cd\u4f5c z = y * y * 3 out = z . mean () print ( z , out ) tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward>) tensor(27., grad_fn=<MeanBackward1>) .requires_grad_( ... ) \u53ef\u4ee5\u6539\u53d8\u73b0\u6709\u5f20\u91cf\u7684 requires_grad \u5c5e\u6027\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u7684\u8bdd\uff0c\u9ed8\u8ba4\u8f93\u5165\u7684flag\u662f False \u3002 a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) False True <SumBackward0 object at 0x000002325360B438> \u68af\u5ea6 \u00b6 \u53cd\u5411\u4f20\u64ad \u56e0\u4e3a out \u662f\u4e00\u4e2a\u7eaf\u91cf\uff08scalar\uff09\uff0c out.backward() \u7b49\u4e8e out.backward(torch.tensor(1)) \u3002 out . backward () print gradients d(out)/dx print ( x . grad ) tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5f97\u5230\u77e9\u9635 4.5 .\u8c03\u7528 out Tensor \u201c o o \u201d. \u5f97\u5230 o = \\frac{1}{4}\\sum_i z_i o = \\frac{1}{4}\\sum_i z_i , z_i = 3(x_i+2)^2 z_i = 3(x_i+2)^2 and z_i\\bigr\\rvert_{x_i=1} = 27 z_i\\bigr\\rvert_{x_i=1} = 27 . \u56e0\u6b64, \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) , hence \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 . \u53ef\u4ee5\u4f7f\u7528 autograd \u505a\u66f4\u591a\u7684\u64cd\u4f5c x = torch . randn ( 3 , requires_grad = True ) y = x * 2 while y . data . norm () < 1000 : y = y * 2 print ( y ) tensor([-920.6895, -115.7301, -867.6995], grad_fn=<MulBackward>) gradients = torch . tensor ([ 0.1 , 1.0 , 0.0001 ], dtype = torch . float ) y . backward ( gradients ) print ( x . grad ) tensor([ 51.2000, 512.0000, 0.0512]) \u5982\u679c .requires_grad=True \u4f46\u662f\u4f60\u53c8\u4e0d\u5e0c\u671b\u8fdb\u884cautograd\u7684\u8ba1\u7b97\uff0c \u90a3\u4e48\u53ef\u4ee5\u5c06\u53d8\u91cf\u5305\u88f9\u5728 with torch.no_grad() \u4e2d: print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) True True False \u7a0d\u540e\u9605\u8bfb: autograd \u548c Function \u7684\u5b98\u65b9\u6587\u6863 https://pytorch.org/docs/autograd","title":"1.3.2 Autograd"},{"location":"tutorial/chapter01_getting-started/1_3_2_autograd_tutorial/#autograd","text":"PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662f autograd \u5305\u3002 \u6211\u4eec\u5148\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b\u8fd9\u4e2a\u5305\uff0c\u7136\u540e\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u3002 autograd \u5305\u4e3a\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u63d0\u4f9b\u4e86\u81ea\u52a8\u6c42\u5bfc\u3002 \u5b83\u662f\u4e00\u4e2a\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad\u662f\u6839\u636e\u4f60\u7684\u4ee3\u7801\u6765\u786e\u5b9a\u5982\u4f55\u8fd0\u884c\uff0c\u5e76\u4e14\u6bcf\u6b21\u8fed\u4ee3\u53ef\u4ee5\u662f\u4e0d\u540c\u7684\u3002 \u793a\u4f8b","title":"Autograd: \u81ea\u52a8\u6c42\u5bfc\u673a\u5236"},{"location":"tutorial/chapter01_getting-started/1_3_2_autograd_tutorial/#tensor","text":"torch.Tensor \u662f\u8fd9\u4e2a\u5305\u7684\u6838\u5fc3\u7c7b\u3002\u5982\u679c\u8bbe\u7f6e .requires_grad \u4e3a True \uff0c\u90a3\u4e48\u5c06\u4f1a\u8ffd\u8e2a\u6240\u6709\u5bf9\u4e8e\u8be5\u5f20\u91cf\u7684\u64cd\u4f5c\u3002 \u5f53\u5b8c\u6210\u8ba1\u7b97\u540e\u901a\u8fc7\u8c03\u7528 .backward() \uff0c\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6\uff0c \u8fd9\u4e2a\u5f20\u91cf\u7684\u6240\u6709\u68af\u5ea6\u5c06\u4f1a\u81ea\u52a8\u79ef\u7d2f\u5230 .grad \u5c5e\u6027\u3002 \u8981\u963b\u6b62\u5f20\u91cf\u8ddf\u8e2a\u5386\u53f2\u8bb0\u5f55\uff0c\u53ef\u4ee5\u8c03\u7528 .detach() \u65b9\u6cd5\u5c06\u5176\u4e0e\u8ba1\u7b97\u5386\u53f2\u8bb0\u5f55\u5206\u79bb\uff0c\u5e76\u7981\u6b62\u8ddf\u8e2a\u5b83\u5c06\u6765\u7684\u8ba1\u7b97\u8bb0\u5f55\u3002 \u4e3a\u4e86\u9632\u6b62\u8ddf\u8e2a\u5386\u53f2\u8bb0\u5f55\uff08\u548c\u4f7f\u7528\u5185\u5b58\uff09\uff0c\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u5305\u88c5\u5728 with torch.no_grad()\uff1a \u4e2d\u3002 \u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u7279\u522b\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709 requires_grad = True \u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4f46\u662f\u6211\u4eec\u4e0d\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u3002 \u5728\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u4e2d\u8fd8\u6709\u53e6\u5916\u4e00\u4e2a\u91cd\u8981\u7684\u7c7b Function . Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor (except for Tensors created by the user - their grad_fn is None ). Tensor \u548c Function \u4e92\u76f8\u8fde\u63a5\u5e76\u751f\u6210\u4e00\u4e2a\u975e\u5faa\u73af\u56fe\uff0c\u5b83\u8868\u793a\u548c\u5b58\u50a8\u4e86\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u3002 \u6bcf\u4e2a\u5f20\u91cf\u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027\uff0c\u8fd9\u4e2a\u5c5e\u6027\u5f15\u7528\u4e86\u4e00\u4e2a\u521b\u5efa\u4e86 Tensor \u7684 Function \uff08\u9664\u975e\u8fd9\u4e2a\u5f20\u91cf\u662f\u7528\u6237\u624b\u52a8\u521b\u5efa\u7684\uff0c\u5373\uff0c\u8fd9\u4e2a\u5f20\u91cf\u7684 grad_fn \u662f None \uff09\u3002 \u5982\u679c\u9700\u8981\u8ba1\u7b97\u5bfc\u6570\uff0c\u4f60\u53ef\u4ee5\u5728 Tensor \u4e0a\u8c03\u7528 .backward() \u3002 \u5982\u679c Tensor \u662f\u4e00\u4e2a\u6807\u91cf\uff08\u5373\u5b83\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u6570\u636e\uff09\u5219\u4e0d\u9700\u8981\u4e3a backward() \u6307\u5b9a\u4efb\u4f55\u53c2\u6570\uff0c \u4f46\u662f\u5982\u679c\u5b83\u6709\u66f4\u591a\u7684\u5143\u7d20\uff0c\u4f60\u9700\u8981\u6307\u5b9a\u4e00\u4e2a gradient \u53c2\u6570\u6765\u5339\u914d\u5f20\u91cf\u7684\u5f62\u72b6\u3002 \u8bd1\u8005\u6ce8\uff1a\u5728\u5176\u4ed6\u7684\u6587\u7ae0\u4e2d\u4f60\u53ef\u80fd\u4f1a\u770b\u5230\u8bf4\u5c06Tensor\u5305\u88f9\u5230Variable\u4e2d\u63d0\u4f9b\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\uff0cVariable \u8fd9\u4e2a\u57280.41\u7248\u4e2d\u5df2\u7ecf\u88ab\u6807\u6ce8\u4e3a\u8fc7\u671f\u4e86\uff0c\u73b0\u5728\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528Tensor\uff0c\u5b98\u65b9\u6587\u6863\u5728\u8fd9\u91cc\uff1a ( https://pytorch.org/docs/stable/autograd.html#variable-deprecated ) \u5177\u4f53\u7684\u540e\u9762\u4f1a\u6709\u8be6\u7ec6\u8bf4\u660e import torch \u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\u5e76\u8bbe\u7f6e requires_grad=True \u7528\u6765\u8ffd\u8e2a\u4ed6\u7684\u8ba1\u7b97\u5386\u53f2 x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) tensor([[1., 1.], [1., 1.]], requires_grad=True) \u5bf9\u5f20\u91cf\u8fdb\u884c\u64cd\u4f5c: y = x + 2 print ( y ) tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward>) \u7ed3\u679c y \u5df2\u7ecf\u88ab\u8ba1\u7b97\u51fa\u6765\u4e86\uff0c\u6240\u4ee5\uff0c grad_fn \u5df2\u7ecf\u88ab\u81ea\u52a8\u751f\u6210\u4e86\u3002 print ( y . grad_fn ) <AddBackward object at 0x00000232535FD860> \u5bf9y\u8fdb\u884c\u4e00\u4e2a\u64cd\u4f5c z = y * y * 3 out = z . mean () print ( z , out ) tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward>) tensor(27., grad_fn=<MeanBackward1>) .requires_grad_( ... ) \u53ef\u4ee5\u6539\u53d8\u73b0\u6709\u5f20\u91cf\u7684 requires_grad \u5c5e\u6027\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u7684\u8bdd\uff0c\u9ed8\u8ba4\u8f93\u5165\u7684flag\u662f False \u3002 a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) False True <SumBackward0 object at 0x000002325360B438>","title":"\u5f20\u91cf\uff08Tensor\uff09"},{"location":"tutorial/chapter01_getting-started/1_3_2_autograd_tutorial/#_1","text":"\u53cd\u5411\u4f20\u64ad \u56e0\u4e3a out \u662f\u4e00\u4e2a\u7eaf\u91cf\uff08scalar\uff09\uff0c out.backward() \u7b49\u4e8e out.backward(torch.tensor(1)) \u3002 out . backward () print gradients d(out)/dx print ( x . grad ) tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5f97\u5230\u77e9\u9635 4.5 .\u8c03\u7528 out Tensor \u201c o o \u201d. \u5f97\u5230 o = \\frac{1}{4}\\sum_i z_i o = \\frac{1}{4}\\sum_i z_i , z_i = 3(x_i+2)^2 z_i = 3(x_i+2)^2 and z_i\\bigr\\rvert_{x_i=1} = 27 z_i\\bigr\\rvert_{x_i=1} = 27 . \u56e0\u6b64, \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) , hence \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 . \u53ef\u4ee5\u4f7f\u7528 autograd \u505a\u66f4\u591a\u7684\u64cd\u4f5c x = torch . randn ( 3 , requires_grad = True ) y = x * 2 while y . data . norm () < 1000 : y = y * 2 print ( y ) tensor([-920.6895, -115.7301, -867.6995], grad_fn=<MulBackward>) gradients = torch . tensor ([ 0.1 , 1.0 , 0.0001 ], dtype = torch . float ) y . backward ( gradients ) print ( x . grad ) tensor([ 51.2000, 512.0000, 0.0512]) \u5982\u679c .requires_grad=True \u4f46\u662f\u4f60\u53c8\u4e0d\u5e0c\u671b\u8fdb\u884cautograd\u7684\u8ba1\u7b97\uff0c \u90a3\u4e48\u53ef\u4ee5\u5c06\u53d8\u91cf\u5305\u88f9\u5728 with torch.no_grad() \u4e2d: print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) True True False \u7a0d\u540e\u9605\u8bfb: autograd \u548c Function \u7684\u5b98\u65b9\u6587\u6863 https://pytorch.org/docs/autograd","title":"\u68af\u5ea6"},{"location":"tutorial/chapter01_getting-started/1_3_3_neural_networks_tutorial/","text":"% matplotlib inline Neural Networks \u00b6 \u4f7f\u7528torch.nn\u5305\u6765\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u3002 \u4e0a\u4e00\u8bb2\u5df2\u7ecf\u8bb2\u8fc7\u4e86 autograd \uff0c nn \u5305\u4f9d\u8d56 autograd \u5305\u6765\u5b9a\u4e49\u6a21\u578b\u5e76\u6c42\u5bfc\u3002 \u4e00\u4e2a nn.Module \u5305\u542b\u5404\u4e2a\u5c42\u548c\u4e00\u4e2a forward(input) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fd4\u56de output \u3002 \u4f8b\u5982\uff1a \u5b83\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\uff0c\u7136\u540e\u4e00\u5c42\u63a5\u7740\u4e00\u5c42\u5730\u4f20\u9012\uff0c\u6700\u540e\u8f93\u51fa\u8ba1\u7b97\u7684\u7ed3\u679c\u3002 \u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a \u5b9a\u4e49\u5305\u542b\u4e00\u4e9b\u53ef\u5b66\u4e60\u7684\u53c2\u6570(\u6216\u8005\u53eb\u6743\u91cd)\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1b \u5728\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3\uff1b \u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u8f93\u5165\uff1b \u8ba1\u7b97\u635f\u5931(\u8f93\u51fa\u7ed3\u679c\u548c\u6b63\u786e\u503c\u7684\u5dee\u503c\u5927\u5c0f)\uff1b \u5c06\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u56de\u7f51\u7edc\u7684\u53c2\u6570\uff1b \u66f4\u65b0\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u4e3b\u8981\u4f7f\u7528\u5982\u4e0b\u7b80\u5355\u7684\u66f4\u65b0\u539f\u5219\uff1a weight = weight - learning_rate * gradient \u5b9a\u4e49\u7f51\u7edc \u00b6 \u5f00\u59cb\u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc\uff1a import torch import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self . conv1 = nn . Conv2d ( 1 , 6 , 5 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) # an affine operation: y = Wx + b self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # Max pooling over a (2, 2) window x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) # If the size is a square you can only specify a single number x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] # all dimensions except the batch dimension num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u5728\u6a21\u578b\u4e2d\u5fc5\u987b\u8981\u5b9a\u4e49 forward \u51fd\u6570\uff0c backward \u51fd\u6570\uff08\u7528\u6765\u8ba1\u7b97\u68af\u5ea6\uff09\u4f1a\u88ab autograd \u81ea\u52a8\u521b\u5efa\u3002 \u53ef\u4ee5\u5728 forward \u51fd\u6570\u4e2d\u4f7f\u7528\u4efb\u4f55\u9488\u5bf9 Tensor \u7684\u64cd\u4f5c\u3002 net.parameters() \u8fd4\u56de\u53ef\u88ab\u5b66\u4e60\u7684\u53c2\u6570\uff08\u6743\u91cd\uff09\u5217\u8868\u548c\u503c params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) # conv1's .weight 10 torch.Size([6, 1, 5, 5]) \u6d4b\u8bd5\u968f\u673a\u8f93\u516532\u00d732\u3002 \u6ce8\uff1a\u8fd9\u4e2a\u7f51\u7edc\uff08LeNet\uff09\u671f\u671b\u7684\u8f93\u5165\u5927\u5c0f\u662f32\u00d732\uff0c\u5982\u679c\u4f7f\u7528MNIST\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\uff0c\u8bf7\u628a\u56fe\u7247\u5927\u5c0f\u91cd\u65b0\u8c03\u6574\u523032\u00d732\u3002 input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) tensor([[-0.0204, -0.0268, -0.0829, 0.1420, -0.0192, 0.1848, 0.0723, -0.0393, -0.0275, 0.0867]], grad_fn=<ThAddmmBackward>) \u5c06\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u7f13\u5b58\u6e05\u96f6\uff0c\u7136\u540e\u8fdb\u884c\u968f\u673a\u68af\u5ea6\u7684\u7684\u53cd\u5411\u4f20\u64ad\uff1a net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) Note ``torch.nn`` \u53ea\u652f\u6301\u5c0f\u6279\u91cf\u8f93\u5165\u3002\u6574\u4e2a ``torch.nn`` \u5305\u90fd\u53ea\u652f\u6301\u5c0f\u6279\u91cf\u6837\u672c\uff0c\u800c\u4e0d\u652f\u6301\u5355\u4e2a\u6837\u672c\u3002 \u4f8b\u5982\uff0c``nn.Conv2d`` \u63a5\u53d7\u4e00\u4e2a4\u7ef4\u7684\u5f20\u91cf\uff0c ``\u6bcf\u4e00\u7ef4\u5206\u522b\u662fsSamples * nChannels * Height * Width\uff08\u6837\u672c\u6570*\u901a\u9053\u6570*\u9ad8*\u5bbd\uff09``\u3002 \u5982\u679c\u4f60\u6709\u5355\u4e2a\u6837\u672c\uff0c\u53ea\u9700\u4f7f\u7528 ``input.unsqueeze(0)`` \u6765\u6dfb\u52a0\u5176\u5b83\u7684\u7ef4\u6570 \u5728\u7ee7\u7eed\u4e4b\u524d\uff0c\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u5230\u76ee\u524d\u4e3a\u6b62\u7528\u5230\u7684\u7c7b\u3002 \u56de\u987e: - torch.Tensor \uff1a\u4e00\u4e2a\u7528\u8fc7\u81ea\u52a8\u8c03\u7528 backward() \u5b9e\u73b0\u652f\u6301\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u7684 \u591a\u7ef4\u6570\u7ec4 \uff0c \u5e76\u4e14\u4fdd\u5b58\u5173\u4e8e\u8fd9\u4e2a\u5411\u91cf\u7684*\u68af\u5ea6* w.r.t. - nn.Module \uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u3002\u5c01\u88c5\u53c2\u6570\u3001\u79fb\u52a8\u5230GPU\u4e0a\u8fd0\u884c\u3001\u5bfc\u51fa\u3001\u52a0\u8f7d\u7b49\u3002 - nn.Parameter \uff1a\u4e00\u79cd\u53d8\u91cf\uff0c\u5f53\u628a\u5b83\u8d4b\u503c\u7ed9\u4e00\u4e2a Module \u65f6\uff0c\u88ab \u81ea\u52a8 \u5730\u6ce8\u518c\u4e3a\u4e00\u4e2a\u53c2\u6570\u3002 - autograd.Function \uff1a\u5b9e\u73b0\u4e00\u4e2a\u81ea\u52a8\u6c42\u5bfc\u64cd\u4f5c\u7684\u524d\u5411\u548c\u53cd\u5411\u5b9a\u4e49\uff0c\u6bcf\u4e2a\u53d8\u91cf\u64cd\u4f5c\u81f3\u5c11\u521b\u5efa\u4e00\u4e2a\u51fd\u6570\u8282\u70b9\uff0c\u6bcf\u4e00\u4e2a Tensor \u7684\u64cd\u4f5c\u90fd\u56de\u521b\u5efa\u4e00\u4e2a\u63a5\u5230\u521b\u5efa Tensor \u548c \u7f16\u7801\u5176\u5386\u53f2 \u7684\u51fd\u6570\u7684 Function \u8282\u70b9\u3002 \u91cd\u70b9\u5982\u4e0b\uff1a - \u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc - \u5904\u7406\u8f93\u5165\uff0c\u8c03\u7528backword \u8fd8\u5269\uff1a - \u8ba1\u7b97\u635f\u5931 - \u66f4\u65b0\u7f51\u7edc\u6743\u91cd \u635f\u5931\u51fd\u6570 \u00b6 \u4e00\u4e2a\u635f\u5931\u51fd\u6570\u63a5\u53d7\u4e00\u5bf9 (output, target) \u4f5c\u4e3a\u8f93\u5165\uff0c\u8ba1\u7b97\u4e00\u4e2a\u503c\u6765\u4f30\u8ba1\u7f51\u7edc\u7684\u8f93\u51fa\u548c\u76ee\u6807\u503c\u76f8\u5dee\u591a\u5c11\u3002 \u8bd1\u8005\u6ce8\uff1aoutput\u4e3a\u7f51\u7edc\u7684\u8f93\u51fa\uff0ctarget\u4e3a\u5b9e\u9645\u503c nn\u5305\u4e2d\u6709\u5f88\u591a\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 \u3002 nn.MSELoss \u662f\u4e00\u4e2a\u6bd4\u8f83\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u8f93\u51fa\u548c\u76ee\u6807\u95f4\u7684**\u5747\u65b9\u8bef\u5dee**\uff0c \u4f8b\u5982\uff1a output = net ( input ) target = torch . randn ( 10 ) # \u968f\u673a\u503c\u4f5c\u4e3a\u6837\u4f8b target = target . view ( 1 , - 1 ) # \u4f7ftarget\u548coutput\u7684shape\u76f8\u540c criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) tensor(1.3172, grad_fn=<MseLossBackward>) Now, if you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this: :: input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss So, when we call loss.backward() , the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient. For illustration, let us follow a few steps backward: print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU \u53cd\u5411\u4f20\u64ad \u00b6 \u8c03\u7528loss.backward()\u83b7\u5f97\u53cd\u5411\u4f20\u64ad\u7684\u8bef\u5dee\u3002 \u4f46\u662f\u5728\u8c03\u7528\u524d\u9700\u8981\u6e05\u9664\u5df2\u5b58\u5728\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u88ab\u7d2f\u52a0\u5230\u5df2\u5b58\u5728\u7684\u68af\u5ea6\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u5c06\u8c03\u7528loss.backward()\uff0c\u5e76\u67e5\u770bconv1\u5c42\u7684\u504f\u5dee\uff08bias\uff09\u9879\u5728\u53cd\u5411\u4f20\u64ad\u524d\u540e\u7684\u68af\u5ea6\u3002 net . zero_grad () # \u6e05\u9664\u68af\u5ea6 print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0074, -0.0249, -0.0107, 0.0326, -0.0017, -0.0059]) \u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570 \u7a0d\u540e\u9605\u8bfb\uff1a nn \u5305\uff0c\u5305\u542b\u4e86\u5404\u79cd\u7528\u6765\u6784\u6210\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u5757\u7684\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\uff0c\u5b8c\u6574\u7684\u6587\u6863\u8bf7\u67e5\u770b here \u3002 \u5269\u4e0b\u7684\u6700\u540e\u4e00\u4ef6\u4e8b: \u65b0\u7f51\u7edc\u7684\u6743\u91cd \u66f4\u65b0\u6743\u91cd \u00b6 \u5728\u5b9e\u8df5\u4e2d\u6700\u7b80\u5355\u7684\u6743\u91cd\u66f4\u65b0\u89c4\u5219\u662f\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff1a ``weight = weight - learning_rate * gradient`` \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684Python\u4ee3\u7801\u5b9e\u73b0\u8fd9\u4e2a\u89c4\u5219\uff1a learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u4f46\u662f\u5f53\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u662f\u60f3\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\u65f6\uff0c\u6bd4\u5982SGD\u3001Nesterov-SGD\u3001Adam\u3001RMSPROP\u7b49\uff0cPyTorch\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u5305 torch.optim \u5b9e\u73b0\u4e86\u6240\u6709\u7684\u8fd9\u4e9b\u89c4\u5219\u3002 \u4f7f\u7528\u5b83\u4eec\u975e\u5e38\u7b80\u5355\uff1a import torch.optim as optim # create your optimizer optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # in your training loop: optimizer . zero_grad () # zero the gradient buffers output = net ( input ) loss = criterion ( output , target ) loss . backward () optimizer . step () # Does the update .. Note:: Observe how gradient buffers had to be manually set to zero using ``optimizer.zero_grad()``. This is because gradients are accumulated as explained in `Backprop`_ section.","title":"1.3.3 Neural Networks"},{"location":"tutorial/chapter01_getting-started/1_3_3_neural_networks_tutorial/#neural-networks","text":"\u4f7f\u7528torch.nn\u5305\u6765\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u3002 \u4e0a\u4e00\u8bb2\u5df2\u7ecf\u8bb2\u8fc7\u4e86 autograd \uff0c nn \u5305\u4f9d\u8d56 autograd \u5305\u6765\u5b9a\u4e49\u6a21\u578b\u5e76\u6c42\u5bfc\u3002 \u4e00\u4e2a nn.Module \u5305\u542b\u5404\u4e2a\u5c42\u548c\u4e00\u4e2a forward(input) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fd4\u56de output \u3002 \u4f8b\u5982\uff1a \u5b83\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\uff0c\u7136\u540e\u4e00\u5c42\u63a5\u7740\u4e00\u5c42\u5730\u4f20\u9012\uff0c\u6700\u540e\u8f93\u51fa\u8ba1\u7b97\u7684\u7ed3\u679c\u3002 \u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a \u5b9a\u4e49\u5305\u542b\u4e00\u4e9b\u53ef\u5b66\u4e60\u7684\u53c2\u6570(\u6216\u8005\u53eb\u6743\u91cd)\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1b \u5728\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3\uff1b \u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u8f93\u5165\uff1b \u8ba1\u7b97\u635f\u5931(\u8f93\u51fa\u7ed3\u679c\u548c\u6b63\u786e\u503c\u7684\u5dee\u503c\u5927\u5c0f)\uff1b \u5c06\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u56de\u7f51\u7edc\u7684\u53c2\u6570\uff1b \u66f4\u65b0\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u4e3b\u8981\u4f7f\u7528\u5982\u4e0b\u7b80\u5355\u7684\u66f4\u65b0\u539f\u5219\uff1a weight = weight - learning_rate * gradient","title":"Neural Networks"},{"location":"tutorial/chapter01_getting-started/1_3_3_neural_networks_tutorial/#_1","text":"\u5f00\u59cb\u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc\uff1a import torch import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self . conv1 = nn . Conv2d ( 1 , 6 , 5 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) # an affine operation: y = Wx + b self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # Max pooling over a (2, 2) window x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) # If the size is a square you can only specify a single number x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] # all dimensions except the batch dimension num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u5728\u6a21\u578b\u4e2d\u5fc5\u987b\u8981\u5b9a\u4e49 forward \u51fd\u6570\uff0c backward \u51fd\u6570\uff08\u7528\u6765\u8ba1\u7b97\u68af\u5ea6\uff09\u4f1a\u88ab autograd \u81ea\u52a8\u521b\u5efa\u3002 \u53ef\u4ee5\u5728 forward \u51fd\u6570\u4e2d\u4f7f\u7528\u4efb\u4f55\u9488\u5bf9 Tensor \u7684\u64cd\u4f5c\u3002 net.parameters() \u8fd4\u56de\u53ef\u88ab\u5b66\u4e60\u7684\u53c2\u6570\uff08\u6743\u91cd\uff09\u5217\u8868\u548c\u503c params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) # conv1's .weight 10 torch.Size([6, 1, 5, 5]) \u6d4b\u8bd5\u968f\u673a\u8f93\u516532\u00d732\u3002 \u6ce8\uff1a\u8fd9\u4e2a\u7f51\u7edc\uff08LeNet\uff09\u671f\u671b\u7684\u8f93\u5165\u5927\u5c0f\u662f32\u00d732\uff0c\u5982\u679c\u4f7f\u7528MNIST\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\uff0c\u8bf7\u628a\u56fe\u7247\u5927\u5c0f\u91cd\u65b0\u8c03\u6574\u523032\u00d732\u3002 input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) tensor([[-0.0204, -0.0268, -0.0829, 0.1420, -0.0192, 0.1848, 0.0723, -0.0393, -0.0275, 0.0867]], grad_fn=<ThAddmmBackward>) \u5c06\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u7f13\u5b58\u6e05\u96f6\uff0c\u7136\u540e\u8fdb\u884c\u968f\u673a\u68af\u5ea6\u7684\u7684\u53cd\u5411\u4f20\u64ad\uff1a net . zero_grad () out . backward ( torch . randn ( 1 , 10 ))","title":"\u5b9a\u4e49\u7f51\u7edc"},{"location":"tutorial/chapter01_getting-started/1_3_3_neural_networks_tutorial/#_2","text":"\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u63a5\u53d7\u4e00\u5bf9 (output, target) \u4f5c\u4e3a\u8f93\u5165\uff0c\u8ba1\u7b97\u4e00\u4e2a\u503c\u6765\u4f30\u8ba1\u7f51\u7edc\u7684\u8f93\u51fa\u548c\u76ee\u6807\u503c\u76f8\u5dee\u591a\u5c11\u3002 \u8bd1\u8005\u6ce8\uff1aoutput\u4e3a\u7f51\u7edc\u7684\u8f93\u51fa\uff0ctarget\u4e3a\u5b9e\u9645\u503c nn\u5305\u4e2d\u6709\u5f88\u591a\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 \u3002 nn.MSELoss \u662f\u4e00\u4e2a\u6bd4\u8f83\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u8f93\u51fa\u548c\u76ee\u6807\u95f4\u7684**\u5747\u65b9\u8bef\u5dee**\uff0c \u4f8b\u5982\uff1a output = net ( input ) target = torch . randn ( 10 ) # \u968f\u673a\u503c\u4f5c\u4e3a\u6837\u4f8b target = target . view ( 1 , - 1 ) # \u4f7ftarget\u548coutput\u7684shape\u76f8\u540c criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) tensor(1.3172, grad_fn=<MseLossBackward>) Now, if you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this: :: input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss So, when we call loss.backward() , the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient. For illustration, let us follow a few steps backward: print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU","title":"\u635f\u5931\u51fd\u6570"},{"location":"tutorial/chapter01_getting-started/1_3_3_neural_networks_tutorial/#_3","text":"\u8c03\u7528loss.backward()\u83b7\u5f97\u53cd\u5411\u4f20\u64ad\u7684\u8bef\u5dee\u3002 \u4f46\u662f\u5728\u8c03\u7528\u524d\u9700\u8981\u6e05\u9664\u5df2\u5b58\u5728\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u88ab\u7d2f\u52a0\u5230\u5df2\u5b58\u5728\u7684\u68af\u5ea6\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u5c06\u8c03\u7528loss.backward()\uff0c\u5e76\u67e5\u770bconv1\u5c42\u7684\u504f\u5dee\uff08bias\uff09\u9879\u5728\u53cd\u5411\u4f20\u64ad\u524d\u540e\u7684\u68af\u5ea6\u3002 net . zero_grad () # \u6e05\u9664\u68af\u5ea6 print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0074, -0.0249, -0.0107, 0.0326, -0.0017, -0.0059]) \u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570 \u7a0d\u540e\u9605\u8bfb\uff1a nn \u5305\uff0c\u5305\u542b\u4e86\u5404\u79cd\u7528\u6765\u6784\u6210\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u5757\u7684\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\uff0c\u5b8c\u6574\u7684\u6587\u6863\u8bf7\u67e5\u770b here \u3002 \u5269\u4e0b\u7684\u6700\u540e\u4e00\u4ef6\u4e8b: \u65b0\u7f51\u7edc\u7684\u6743\u91cd","title":"\u53cd\u5411\u4f20\u64ad"},{"location":"tutorial/chapter01_getting-started/1_3_3_neural_networks_tutorial/#_4","text":"\u5728\u5b9e\u8df5\u4e2d\u6700\u7b80\u5355\u7684\u6743\u91cd\u66f4\u65b0\u89c4\u5219\u662f\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff1a ``weight = weight - learning_rate * gradient`` \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684Python\u4ee3\u7801\u5b9e\u73b0\u8fd9\u4e2a\u89c4\u5219\uff1a learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u4f46\u662f\u5f53\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u662f\u60f3\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\u65f6\uff0c\u6bd4\u5982SGD\u3001Nesterov-SGD\u3001Adam\u3001RMSPROP\u7b49\uff0cPyTorch\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u5305 torch.optim \u5b9e\u73b0\u4e86\u6240\u6709\u7684\u8fd9\u4e9b\u89c4\u5219\u3002 \u4f7f\u7528\u5b83\u4eec\u975e\u5e38\u7b80\u5355\uff1a import torch.optim as optim # create your optimizer optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # in your training loop: optimizer . zero_grad () # zero the gradient buffers output = net ( input ) loss = criterion ( output , target ) loss . backward () optimizer . step () # Does the update .. Note:: Observe how gradient buffers had to be manually set to zero using ``optimizer.zero_grad()``. This is because gradients are accumulated as explained in `Backprop`_ section.","title":"\u66f4\u65b0\u6743\u91cd"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/","text":"% matplotlib inline \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 \u00b6 \u4e0a\u4e00\u8bb2\u4e2d\u5df2\u7ecf\u770b\u5230\u5982\u4f55\u53bb\u5b9a\u4e49\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u5931\u503c\u548c\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd\u3002 \u4f60\u73b0\u5728\u53ef\u80fd\u5728\u60f3\u4e0b\u4e00\u6b65\u3002 \u5173\u4e8e\u6570\u636e\uff1f \u00b6 \u4e00\u822c\u60c5\u51b5\u4e0b\u5904\u7406\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6570\u636e\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684Python\u5305\u6765\u52a0\u8f7d\u6570\u636e\u5230\u4e00\u4e2anumpy\u6570\u7ec4\u4e2d\u3002 \u7136\u540e\u628a\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210 torch.*Tensor \u3002 \u56fe\u50cf\u53ef\u4ee5\u4f7f\u7528 Pillow, OpenCV \u97f3\u9891\u53ef\u4ee5\u4f7f\u7528 scipy, librosa \u6587\u672c\u53ef\u4ee5\u4f7f\u7528\u539f\u59cbPython\u548cCython\u6765\u52a0\u8f7d\uff0c\u6216\u8005\u4f7f\u7528 NLTK\u6216 SpaCy \u5904\u7406 \u7279\u522b\u7684\uff0c\u5bf9\u4e8e\u56fe\u50cf\u4efb\u52a1\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305 torchvision \uff0c\u5b83\u5305\u542b\u4e86\u5904\u7406\u4e00\u4e9b\u57fa\u672c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u5305\u62ec Imagenet, CIFAR10, MNIST \u7b49\u3002\u9664\u4e86\u6570\u636e\u52a0\u8f7d\u4ee5\u5916\uff0c torchvision \u8fd8\u5305\u542b\u4e86\u56fe\u50cf\u8f6c\u6362\u5668\uff0c torchvision.datasets \u548c torch.utils.data.DataLoader \u3002 torchvision \u5305\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u4fbf\u5229\uff0c\u4e5f\u907f\u514d\u4e86\u4ee3\u7801\u7684\u91cd\u590d\u3002 \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\uff0c\u5b83\u6709\u5982\u4e0b10\u4e2a\u7c7b\u522b \uff1a\u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019, \u2018truck\u2019\u3002CIFAR-10\u7684\u56fe\u50cf\u90fd\u662f 3x32x32\u5927\u5c0f\u7684\uff0c\u5373\uff0c3\u989c\u8272\u901a\u9053\uff0c32x32\u50cf\u7d20\u3002 \u8bad\u7ec3\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668 \u00b6 \u4f9d\u6b21\u6309\u7167\u4e0b\u5217\u987a\u5e8f\u8fdb\u884c\uff1a \u4f7f\u7528 torchvision \u52a0\u8f7d\u548c\u5f52\u4e00\u5316CIFAR10\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u7f51\u7edc \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u7f51\u7edc 1. \u8bfb\u53d6\u548c\u5f52\u4e00\u5316 CIFAR10 \u00b6 \u4f7f\u7528 torchvision \u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u5730\u52a0\u8f7dCIFAR10\u3002 import torch import torchvision import torchvision.transforms as transforms torchvision\u7684\u8f93\u51fa\u662f[0,1]\u7684PILImage\u56fe\u50cf\uff0c\u6211\u4eec\u628a\u5b83\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u8303\u56f4\u4e3a[-1, 1]\u7684\u5f20\u91cf\u3002 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 170M/170M [20:39<00:00, 155kB/s] Files already downloaded and verified \u6211\u4eec\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\u3002 import matplotlib.pyplot as plt import numpy as np # \u5c55\u793a\u56fe\u50cf\u7684\u51fd\u6570 def imshow ( img ): img = img / 2 + 0.5 # unnormalize npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # \u83b7\u53d6\u968f\u673a\u6570\u636e dataiter = iter ( trainloader ) images , labels = dataiter . next () # \u5c55\u793a\u56fe\u50cf imshow ( torchvision . utils . make_grid ( images )) # \u663e\u793a\u56fe\u50cf\u6807\u7b7e print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 171MB [20:51, 155kB/s] cat car cat ship 2. \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u00b6 \u4ece\u4e4b\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u4e00\u8282\u590d\u5236\u795e\u7ecf\u7f51\u7edc\u4ee3\u7801\uff0c\u5e76\u4fee\u6539\u4e3a\u8f93\u51653\u901a\u9053\u56fe\u50cf\u3002 import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () 3. \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668 \u00b6 \u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u5e26\u52a8\u91cf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002 import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) 4. \u8bad\u7ec3\u7f51\u8def \u00b6 \u6709\u8da3\u7684\u65f6\u523b\u5f00\u59cb\u4e86\u3002 \u6211\u4eec\u53ea\u9700\u5728\u6570\u636e\u8fed\u4ee3\u5668\u4e0a\u5faa\u73af\uff0c\u5c06\u6570\u636e\u8f93\u5165\u7ed9\u7f51\u7edc\uff0c\u5e76\u4f18\u5316\u3002 for epoch in range ( 2 ): # \u591a\u6279\u6b21\u5faa\u73af running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # \u83b7\u53d6\u8f93\u5165 inputs , labels = data # \u68af\u5ea6\u7f6e0 optimizer . zero_grad () # \u6b63\u5411\u4f20\u64ad\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316 outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # \u6253\u5370\u72b6\u6001\u4fe1\u606f running_loss += loss . item () if i % 2000 == 1999 : # \u6bcf2000\u6279\u6b21\u6253\u5370\u4e00\u6b21 print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Finished Training' ) 5. \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u7f51\u7edc \u00b6 \u6211\u4eec\u5728\u6574\u4e2a\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u4e862\u6b21\u8bad\u7ec3\uff0c\u4f46\u662f\u6211\u4eec\u9700\u8981\u68c0\u67e5\u7f51\u7edc\u662f\u5426\u4ece\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5230\u6709\u7528\u7684\u4e1c\u897f\u3002 \u901a\u8fc7\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u7c7b\u522b\u6807\u7b7e\u4e0e\u5b9e\u9645\u60c5\u51b5\u6807\u7b7e\u8fdb\u884c\u5bf9\u6bd4\u6765\u8fdb\u884c\u68c0\u6d4b\u3002 \u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u6211\u4eec\u628a\u8be5\u6837\u672c\u6dfb\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u5217\u8868\u3002 \u7b2c\u4e00\u6b65\uff0c\u663e\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u56fe\u7247\u5e76\u719f\u6089\u56fe\u7247\u5185\u5bb9\u3002 dataiter = iter ( testloader ) images , labels = dataiter . next () # \u663e\u793a\u56fe\u7247 imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) GroundTruth: cat ship ship plane \u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4ee5\u4e0a\u56fe\u7247\u662f\u4ec0\u4e48\u3002 outputs = net ( images ) \u8f93\u51fa\u662f10\u4e2a\u6807\u7b7e\u7684\u80fd\u91cf\u3002 \u4e00\u4e2a\u7c7b\u522b\u7684\u80fd\u91cf\u8d8a\u5927\uff0c\u795e\u7ecf\u7f51\u7edc\u8d8a\u8ba4\u4e3a\u5b83\u662f\u8fd9\u4e2a\u7c7b\u522b\u3002\u6240\u4ee5\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u7684\u6807\u7b7e\u3002 _ , predicted = torch . max ( outputs , 1 ) print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) Predicted: plane plane plane plane \u7ed3\u679c\u770b\u6765\u4e0d\u9519\u3002 \u63a5\u4e0b\u6765\u8ba9\u770b\u770b\u7f51\u7edc\u5728\u6574\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u7ed3\u679c\u5982\u4f55\u3002 correct = 0 total = 0 with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the 10000 test images: 9 % \u7ed3\u679c\u770b\u8d77\u6765\u4e0d\u9519\uff0c\u81f3\u5c11\u6bd4\u968f\u673a\u9009\u62e9\u8981\u597d\uff0c\u968f\u673a\u9009\u62e9\u7684\u6b63\u786e\u7387\u4e3a10%\u3002 \u4f3c\u4e4e\u7f51\u7edc\u5b66\u4e60\u5230\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002 \u5728\u8bc6\u522b\u54ea\u4e00\u4e2a\u7c7b\u7684\u65f6\u5019\u597d\uff0c\u54ea\u4e00\u4e2a\u4e0d\u597d\u5462\uff1f class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of plane : 99 % Accuracy of car : 0 % Accuracy of bird : 0 % Accuracy of cat : 0 % Accuracy of deer : 0 % Accuracy of dog : 0 % Accuracy of frog : 0 % Accuracy of horse : 0 % Accuracy of ship : 0 % Accuracy of truck : 0 % \u4e0b\u4e00\u6b65? \u6211\u4eec\u5982\u4f55\u5728GPU\u4e0a\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u5462\uff1f \u5728GPU\u4e0a\u8bad\u7ec3 \u00b6 \u628a\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u79fb\u52a8\u5230GPU\u4e0a\u8bad\u7ec3\u5c31\u50cf\u628a\u4e00\u4e2aTensor\u8f6c\u6362GPU\u4e0a\u4e00\u6837\u7b80\u5355\u3002\u5e76\u4e14\u8fd9\u4e2a\u64cd\u4f5c\u4f1a\u9012\u5f52\u904d\u5386\u6709\u6240\u6a21\u5757\uff0c\u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\u3002 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # \u786e\u8ba4\u6211\u4eec\u7684\u7535\u8111\u652f\u6301CUDA\uff0c\u7136\u540e\u663e\u793aCUDA\u4fe1\u606f\uff1a print ( device ) \u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u5b9a device \u662fCUDA\u8bbe\u5907\u3002 \u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\u5e76\u5c06\u6a21\u5757\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a \u8f6c\u6362\u6210CUDA\u5f20\u91cf\uff1a net . to ( device ) \u8bb0\u4f4f\uff1ainputs \u548c targets \u4e5f\u8981\u8f6c\u6362\u3002 inputs , labels = inputs . to ( device ), labels . to ( device ) \u4e3a\u4ec0\u4e48\u6211\u4eec\u6ca1\u6ce8\u610f\u5230GPU\u7684\u901f\u5ea6\u63d0\u5347\u5f88\u591a\uff1f\u90a3\u662f\u56e0\u4e3a\u7f51\u7edc\u975e\u5e38\u7684\u5c0f\u3002 \u5b9e\u8df5: \u5c1d\u8bd5\u589e\u52a0\u4f60\u7684\u7f51\u7edc\u7684\u5bbd\u5ea6\uff08\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u7b2c2\u4e2a\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u5b83\u4eec\u9700\u8981\u662f\u76f8\u540c\u7684\u6570\u5b57\uff09\uff0c\u770b\u770b\u4f60\u5f97\u5230\u4e86\u4ec0\u4e48\u6837\u7684\u52a0\u901f\u3002 \u5b9e\u73b0\u7684\u76ee\u6807 : \u6df1\u5165\u4e86\u89e3\u4e86PyTorch\u7684\u5f20\u91cf\u5e93\u548c\u795e\u7ecf\u7f51\u7edc \u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u7f51\u7edc\u6765\u5206\u7c7b\u56fe\u7247 \u8bd1\u8005\u6ce8\uff1a\u540e\u9762\u6211\u4eec\u6559\u7a0b\u4f1a\u8bad\u7ec3\u4e00\u4e2a\u771f\u6b63\u7684\u7f51\u7edc\uff0c\u4f7f\u8bc6\u522b\u7387\u8fbe\u523090%\u4ee5\u4e0a\u3002 \u591aGPU\u8bad\u7ec3 \u00b6 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u6240\u6709\u7684GPU\u5f97\u5230\u66f4\u5927\u7684\u52a0\u901f\uff0c \u8bf7\u67e5\u770b \u6570\u636e\u5e76\u884c\u5904\u7406 \u3002 \u4e0b\u4e00\u6b65\uff1f \u00b6 :doc: \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u73a9\u7535\u5b50\u6e38\u620f </intermediate/reinforcement_q_learning> \u5728ImageNet\u4e0a\u8bad\u7ec3\u6700\u597d\u7684ResNet \u4f7f\u7528\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u6765\u8bad\u7ec3\u4e00\u4e2a\u4eba\u8138\u751f\u6210\u5668 \u4f7f\u7528LSTM\u7f51\u7edc\u8bad\u7ec3\u4e00\u4e2a\u5b57\u7b26\u7ea7\u7684\u8bed\u8a00\u6a21\u578b \u66f4\u591a\u793a\u4f8b \u66f4\u591a\u6559\u7a0b \u5728\u8bba\u575b\u4e0a\u8ba8\u8bbaPyTorch Slack\u4e0a\u4e0e\u5176\u4ed6\u7528\u6237\u8ba8\u8bba","title":"1.3.4 Classifier"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#_1","text":"\u4e0a\u4e00\u8bb2\u4e2d\u5df2\u7ecf\u770b\u5230\u5982\u4f55\u53bb\u5b9a\u4e49\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u5931\u503c\u548c\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd\u3002 \u4f60\u73b0\u5728\u53ef\u80fd\u5728\u60f3\u4e0b\u4e00\u6b65\u3002","title":"\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#_2","text":"\u4e00\u822c\u60c5\u51b5\u4e0b\u5904\u7406\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6570\u636e\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684Python\u5305\u6765\u52a0\u8f7d\u6570\u636e\u5230\u4e00\u4e2anumpy\u6570\u7ec4\u4e2d\u3002 \u7136\u540e\u628a\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210 torch.*Tensor \u3002 \u56fe\u50cf\u53ef\u4ee5\u4f7f\u7528 Pillow, OpenCV \u97f3\u9891\u53ef\u4ee5\u4f7f\u7528 scipy, librosa \u6587\u672c\u53ef\u4ee5\u4f7f\u7528\u539f\u59cbPython\u548cCython\u6765\u52a0\u8f7d\uff0c\u6216\u8005\u4f7f\u7528 NLTK\u6216 SpaCy \u5904\u7406 \u7279\u522b\u7684\uff0c\u5bf9\u4e8e\u56fe\u50cf\u4efb\u52a1\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305 torchvision \uff0c\u5b83\u5305\u542b\u4e86\u5904\u7406\u4e00\u4e9b\u57fa\u672c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u5305\u62ec Imagenet, CIFAR10, MNIST \u7b49\u3002\u9664\u4e86\u6570\u636e\u52a0\u8f7d\u4ee5\u5916\uff0c torchvision \u8fd8\u5305\u542b\u4e86\u56fe\u50cf\u8f6c\u6362\u5668\uff0c torchvision.datasets \u548c torch.utils.data.DataLoader \u3002 torchvision \u5305\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u4fbf\u5229\uff0c\u4e5f\u907f\u514d\u4e86\u4ee3\u7801\u7684\u91cd\u590d\u3002 \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\uff0c\u5b83\u6709\u5982\u4e0b10\u4e2a\u7c7b\u522b \uff1a\u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019, \u2018truck\u2019\u3002CIFAR-10\u7684\u56fe\u50cf\u90fd\u662f 3x32x32\u5927\u5c0f\u7684\uff0c\u5373\uff0c3\u989c\u8272\u901a\u9053\uff0c32x32\u50cf\u7d20\u3002","title":"\u5173\u4e8e\u6570\u636e\uff1f"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#_3","text":"\u4f9d\u6b21\u6309\u7167\u4e0b\u5217\u987a\u5e8f\u8fdb\u884c\uff1a \u4f7f\u7528 torchvision \u52a0\u8f7d\u548c\u5f52\u4e00\u5316CIFAR10\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u7f51\u7edc \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u7f51\u7edc","title":"\u8bad\u7ec3\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#1-cifar10","text":"\u4f7f\u7528 torchvision \u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u5730\u52a0\u8f7dCIFAR10\u3002 import torch import torchvision import torchvision.transforms as transforms torchvision\u7684\u8f93\u51fa\u662f[0,1]\u7684PILImage\u56fe\u50cf\uff0c\u6211\u4eec\u628a\u5b83\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u8303\u56f4\u4e3a[-1, 1]\u7684\u5f20\u91cf\u3002 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 170M/170M [20:39<00:00, 155kB/s] Files already downloaded and verified \u6211\u4eec\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\u3002 import matplotlib.pyplot as plt import numpy as np # \u5c55\u793a\u56fe\u50cf\u7684\u51fd\u6570 def imshow ( img ): img = img / 2 + 0.5 # unnormalize npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # \u83b7\u53d6\u968f\u673a\u6570\u636e dataiter = iter ( trainloader ) images , labels = dataiter . next () # \u5c55\u793a\u56fe\u50cf imshow ( torchvision . utils . make_grid ( images )) # \u663e\u793a\u56fe\u50cf\u6807\u7b7e print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 171MB [20:51, 155kB/s] cat car cat ship","title":"1. \u8bfb\u53d6\u548c\u5f52\u4e00\u5316 CIFAR10"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#2","text":"\u4ece\u4e4b\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u4e00\u8282\u590d\u5236\u795e\u7ecf\u7f51\u7edc\u4ee3\u7801\uff0c\u5e76\u4fee\u6539\u4e3a\u8f93\u51653\u901a\u9053\u56fe\u50cf\u3002 import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net ()","title":"2. \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#3","text":"\u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u5e26\u52a8\u91cf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002 import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 )","title":"3. \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#4","text":"\u6709\u8da3\u7684\u65f6\u523b\u5f00\u59cb\u4e86\u3002 \u6211\u4eec\u53ea\u9700\u5728\u6570\u636e\u8fed\u4ee3\u5668\u4e0a\u5faa\u73af\uff0c\u5c06\u6570\u636e\u8f93\u5165\u7ed9\u7f51\u7edc\uff0c\u5e76\u4f18\u5316\u3002 for epoch in range ( 2 ): # \u591a\u6279\u6b21\u5faa\u73af running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # \u83b7\u53d6\u8f93\u5165 inputs , labels = data # \u68af\u5ea6\u7f6e0 optimizer . zero_grad () # \u6b63\u5411\u4f20\u64ad\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316 outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # \u6253\u5370\u72b6\u6001\u4fe1\u606f running_loss += loss . item () if i % 2000 == 1999 : # \u6bcf2000\u6279\u6b21\u6253\u5370\u4e00\u6b21 print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Finished Training' )","title":"4. \u8bad\u7ec3\u7f51\u8def"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#5","text":"\u6211\u4eec\u5728\u6574\u4e2a\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u4e862\u6b21\u8bad\u7ec3\uff0c\u4f46\u662f\u6211\u4eec\u9700\u8981\u68c0\u67e5\u7f51\u7edc\u662f\u5426\u4ece\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5230\u6709\u7528\u7684\u4e1c\u897f\u3002 \u901a\u8fc7\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u7c7b\u522b\u6807\u7b7e\u4e0e\u5b9e\u9645\u60c5\u51b5\u6807\u7b7e\u8fdb\u884c\u5bf9\u6bd4\u6765\u8fdb\u884c\u68c0\u6d4b\u3002 \u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u6211\u4eec\u628a\u8be5\u6837\u672c\u6dfb\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u5217\u8868\u3002 \u7b2c\u4e00\u6b65\uff0c\u663e\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u56fe\u7247\u5e76\u719f\u6089\u56fe\u7247\u5185\u5bb9\u3002 dataiter = iter ( testloader ) images , labels = dataiter . next () # \u663e\u793a\u56fe\u7247 imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) GroundTruth: cat ship ship plane \u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4ee5\u4e0a\u56fe\u7247\u662f\u4ec0\u4e48\u3002 outputs = net ( images ) \u8f93\u51fa\u662f10\u4e2a\u6807\u7b7e\u7684\u80fd\u91cf\u3002 \u4e00\u4e2a\u7c7b\u522b\u7684\u80fd\u91cf\u8d8a\u5927\uff0c\u795e\u7ecf\u7f51\u7edc\u8d8a\u8ba4\u4e3a\u5b83\u662f\u8fd9\u4e2a\u7c7b\u522b\u3002\u6240\u4ee5\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u7684\u6807\u7b7e\u3002 _ , predicted = torch . max ( outputs , 1 ) print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) Predicted: plane plane plane plane \u7ed3\u679c\u770b\u6765\u4e0d\u9519\u3002 \u63a5\u4e0b\u6765\u8ba9\u770b\u770b\u7f51\u7edc\u5728\u6574\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u7ed3\u679c\u5982\u4f55\u3002 correct = 0 total = 0 with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) Accuracy of the network on the 10000 test images: 9 % \u7ed3\u679c\u770b\u8d77\u6765\u4e0d\u9519\uff0c\u81f3\u5c11\u6bd4\u968f\u673a\u9009\u62e9\u8981\u597d\uff0c\u968f\u673a\u9009\u62e9\u7684\u6b63\u786e\u7387\u4e3a10%\u3002 \u4f3c\u4e4e\u7f51\u7edc\u5b66\u4e60\u5230\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002 \u5728\u8bc6\u522b\u54ea\u4e00\u4e2a\u7c7b\u7684\u65f6\u5019\u597d\uff0c\u54ea\u4e00\u4e2a\u4e0d\u597d\u5462\uff1f class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) Accuracy of plane : 99 % Accuracy of car : 0 % Accuracy of bird : 0 % Accuracy of cat : 0 % Accuracy of deer : 0 % Accuracy of dog : 0 % Accuracy of frog : 0 % Accuracy of horse : 0 % Accuracy of ship : 0 % Accuracy of truck : 0 % \u4e0b\u4e00\u6b65? \u6211\u4eec\u5982\u4f55\u5728GPU\u4e0a\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u5462\uff1f","title":"5. \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u7f51\u7edc"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#gpu","text":"\u628a\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u79fb\u52a8\u5230GPU\u4e0a\u8bad\u7ec3\u5c31\u50cf\u628a\u4e00\u4e2aTensor\u8f6c\u6362GPU\u4e0a\u4e00\u6837\u7b80\u5355\u3002\u5e76\u4e14\u8fd9\u4e2a\u64cd\u4f5c\u4f1a\u9012\u5f52\u904d\u5386\u6709\u6240\u6a21\u5757\uff0c\u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\u3002 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # \u786e\u8ba4\u6211\u4eec\u7684\u7535\u8111\u652f\u6301CUDA\uff0c\u7136\u540e\u663e\u793aCUDA\u4fe1\u606f\uff1a print ( device ) \u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u5b9a device \u662fCUDA\u8bbe\u5907\u3002 \u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\u5e76\u5c06\u6a21\u5757\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a \u8f6c\u6362\u6210CUDA\u5f20\u91cf\uff1a net . to ( device ) \u8bb0\u4f4f\uff1ainputs \u548c targets \u4e5f\u8981\u8f6c\u6362\u3002 inputs , labels = inputs . to ( device ), labels . to ( device ) \u4e3a\u4ec0\u4e48\u6211\u4eec\u6ca1\u6ce8\u610f\u5230GPU\u7684\u901f\u5ea6\u63d0\u5347\u5f88\u591a\uff1f\u90a3\u662f\u56e0\u4e3a\u7f51\u7edc\u975e\u5e38\u7684\u5c0f\u3002 \u5b9e\u8df5: \u5c1d\u8bd5\u589e\u52a0\u4f60\u7684\u7f51\u7edc\u7684\u5bbd\u5ea6\uff08\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u7b2c2\u4e2a\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u5b83\u4eec\u9700\u8981\u662f\u76f8\u540c\u7684\u6570\u5b57\uff09\uff0c\u770b\u770b\u4f60\u5f97\u5230\u4e86\u4ec0\u4e48\u6837\u7684\u52a0\u901f\u3002 \u5b9e\u73b0\u7684\u76ee\u6807 : \u6df1\u5165\u4e86\u89e3\u4e86PyTorch\u7684\u5f20\u91cf\u5e93\u548c\u795e\u7ecf\u7f51\u7edc \u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u7f51\u7edc\u6765\u5206\u7c7b\u56fe\u7247 \u8bd1\u8005\u6ce8\uff1a\u540e\u9762\u6211\u4eec\u6559\u7a0b\u4f1a\u8bad\u7ec3\u4e00\u4e2a\u771f\u6b63\u7684\u7f51\u7edc\uff0c\u4f7f\u8bc6\u522b\u7387\u8fbe\u523090%\u4ee5\u4e0a\u3002","title":"\u5728GPU\u4e0a\u8bad\u7ec3"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#gpu_1","text":"\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u6240\u6709\u7684GPU\u5f97\u5230\u66f4\u5927\u7684\u52a0\u901f\uff0c \u8bf7\u67e5\u770b \u6570\u636e\u5e76\u884c\u5904\u7406 \u3002","title":"\u591aGPU\u8bad\u7ec3"},{"location":"tutorial/chapter01_getting-started/1_3_4_cifar10_tutorial/#_4","text":":doc: \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u73a9\u7535\u5b50\u6e38\u620f </intermediate/reinforcement_q_learning> \u5728ImageNet\u4e0a\u8bad\u7ec3\u6700\u597d\u7684ResNet \u4f7f\u7528\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u6765\u8bad\u7ec3\u4e00\u4e2a\u4eba\u8138\u751f\u6210\u5668 \u4f7f\u7528LSTM\u7f51\u7edc\u8bad\u7ec3\u4e00\u4e2a\u5b57\u7b26\u7ea7\u7684\u8bed\u8a00\u6a21\u578b \u66f4\u591a\u793a\u4f8b \u66f4\u591a\u6559\u7a0b \u5728\u8bba\u575b\u4e0a\u8ba8\u8bbaPyTorch Slack\u4e0a\u4e0e\u5176\u4ed6\u7528\u6237\u8ba8\u8bba","title":"\u4e0b\u4e00\u6b65\uff1f"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/","text":"% matplotlib inline \u6570\u636e\u5e76\u884c\uff08\u9009\u8bfb\uff09 \u00b6 Authors : Sung Kim and Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u91cc\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528 DataParallel \u6765\u4f7f\u7528\u591aGPU\u3002 PyTorch\u975e\u5e38\u5bb9\u6613\u5c31\u53ef\u4ee5\u4f7f\u7528\u591aGPU\uff0c\u7528\u5982\u4e0b\u65b9\u5f0f\u628a\u4e00\u4e2a\u6a21\u578b\u653e\u5230GPU\u4e0a\uff1a device = torch . device ( \"cuda:0\" ) model . to ( device ) GPU: \u7136\u540e\u590d\u5236\u6240\u6709\u7684\u5f20\u91cf\u5230GPU\u4e0a\uff1a mytensor = my_tensor . to ( device ) \u8bf7\u6ce8\u610f\uff0c\u53ea\u8c03\u7528 my_tensor.to(device) \u5e76\u6ca1\u6709\u590d\u5236\u5f20\u91cf\u5230GPU\u4e0a\uff0c\u800c\u662f\u8fd4\u56de\u4e86\u4e00\u4e2acopy\u3002\u6240\u4ee5\u4f60\u9700\u8981\u628a\u5b83\u8d4b\u503c\u7ed9\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u5e76\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u5f20\u91cf\u3002 \u5728\u591aGPU\u4e0a\u6267\u884c\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u662f\u81ea\u7136\u800c\u7136\u7684\u4e8b\u3002 \u4f46\u662fPyTorch\u9ed8\u8ba4\u5c06\u53ea\u4f7f\u7528\u4e00\u4e2aGPU\u3002 \u4f7f\u7528 DataParallel \u53ef\u4ee5\u8f7b\u6613\u7684\u8ba9\u6a21\u578b\u5e76\u884c\u8fd0\u884c\u5728\u591a\u4e2aGPU\u4e0a\u3002 model = nn . DataParallel ( model ) \u8fd9\u624d\u662f\u8fd9\u7bc7\u6559\u7a0b\u7684\u6838\u5fc3\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u66f4\u8be6\u7ec6\u7684\u4ecb\u7ecd\u5b83\u3002 \u5bfc\u5165\u548c\u53c2\u6570 \u00b6 \u5bfc\u5165PyTorch\u6a21\u5757\u548c\u5b9a\u4e49\u53c2\u6570\u3002 import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader # Parameters and DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 Device device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) \u865a\u62df\u6570\u636e\u96c6 \u00b6 \u5236\u4f5c\u4e00\u4e2a\u865a\u62df\uff08\u968f\u673a\uff09\u6570\u636e\u96c6\uff0c \u4f60\u53ea\u9700\u5b9e\u73b0 __getitem__ class RandomDataset ( Dataset ): def __init__ ( self , size , length ): self . len = length self . data = torch . randn ( length , size ) def __getitem__ ( self , index ): return self . data [ index ] def __len__ ( self ): return self . len rand_loader = DataLoader ( dataset = RandomDataset ( input_size , data_size ), batch_size = batch_size , shuffle = True ) \u7b80\u5355\u6a21\u578b \u00b6 \u4f5c\u4e3a\u6f14\u793a\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\uff0c\u6267\u884c\u4e00\u4e2a\u7ebf\u6027\u64cd\u4f5c\uff0c\u7136\u540e\u5f97\u5230\u7ed3\u679c\u3002 \u8bf4\u660e\uff1a DataParallel \u80fd\u5728\u4efb\u4f55\u6a21\u578b\uff08CNN\uff0cRNN\uff0cCapsule Net\u7b49\uff09\u4e0a\u4f7f\u7528\u3002 \u6211\u4eec\u5728\u6a21\u578b\u5185\u90e8\u653e\u7f6e\u4e86\u4e00\u6761\u6253\u5370\u8bed\u53e5\u6765\u6253\u5370\u8f93\u5165\u548c\u8f93\u51fa\u5411\u91cf\u7684\u5927\u5c0f\u3002 \u8bf7\u6ce8\u610f\u6279\u6b21\u7684\u79e9\u4e3a0\u65f6\u6253\u5370\u7684\u5185\u5bb9\u3002 class Model ( nn . Module ): # Our model def __init__ ( self , input_size , output_size ): super ( Model , self ) . __init__ () self . fc = nn . Linear ( input_size , output_size ) def forward ( self , input ): output = self . fc ( input ) print ( \" \\t In Model: input size\" , input . size (), \"output size\" , output . size ()) return output \u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c \u00b6 \u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u548c\u68c0\u6d4b\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002 \u5982\u679c\u6709\u591a\u4e2aGPU\uff0c\u4f7f\u7528 nn.DataParallel \u6765\u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002 \u7136\u540e\u901a\u8fc7m model.to(device) \u628a\u6a21\u578b\u653e\u5230GPU\u4e0a\u3002 model = Model ( input_size , output_size ) if torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn . DataParallel ( model ) model . to ( device ) Model( (fc): Linear(in_features=5, out_features=2, bias=True) ) \u8fd0\u884c\u6a21\u578b \u00b6 \u73b0\u5728\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 for data in rand_loader : input = data . to ( device ) output = model ( input ) print ( \"Outside: input size\" , input . size (), \"output_size\" , output . size ()) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u7ed3\u679c \u00b6 \u5f53\u6ca1\u6709\u6216\u8005\u53ea\u6709\u4e00\u4e2aGPU\u65f6\uff0c\u5bf930\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\u8fdb\u884c\u6279\u5904\u7406\uff0c\u5f97\u5230\u4e86\u671f\u671b\u7684\u4e00\u6837\u5f97\u523030\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u4f46\u662f\u5982\u679c\u4f60\u6709\u591a\u4e2aGPU\uff0c\u4f60\u5f97\u5230\u5982\u4e0b\u7684\u7ed3\u679c\u3002 2 GPUs ~ If you have 2, you will see: .. code:: bash # on 2 GPUs Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3 GPUs ~ If you have 3 GPUs, you will see: .. code:: bash Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8 GPUs ~~ If you have 8, you will see: .. code:: bash Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u603b\u7ed3 \u00b6 DataParallel\u4f1a\u81ea\u52a8\u7684\u5212\u5206\u6570\u636e\uff0c\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002 \u5e76\u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u4f5c\u4e1a\u540e\uff0c\u6536\u96c6\u5408\u5e76\u7ed3\u679c\u5e76\u8fd4\u56de\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u770b\u8fd9\u91cc\uff1a https://pytorch.org/tutorials/beginner/former _torchies/parallelism_tutorial.html.","title":"1.3.5 Data Parallelism"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_1","text":"Authors : Sung Kim and Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u91cc\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528 DataParallel \u6765\u4f7f\u7528\u591aGPU\u3002 PyTorch\u975e\u5e38\u5bb9\u6613\u5c31\u53ef\u4ee5\u4f7f\u7528\u591aGPU\uff0c\u7528\u5982\u4e0b\u65b9\u5f0f\u628a\u4e00\u4e2a\u6a21\u578b\u653e\u5230GPU\u4e0a\uff1a device = torch . device ( \"cuda:0\" ) model . to ( device ) GPU: \u7136\u540e\u590d\u5236\u6240\u6709\u7684\u5f20\u91cf\u5230GPU\u4e0a\uff1a mytensor = my_tensor . to ( device ) \u8bf7\u6ce8\u610f\uff0c\u53ea\u8c03\u7528 my_tensor.to(device) \u5e76\u6ca1\u6709\u590d\u5236\u5f20\u91cf\u5230GPU\u4e0a\uff0c\u800c\u662f\u8fd4\u56de\u4e86\u4e00\u4e2acopy\u3002\u6240\u4ee5\u4f60\u9700\u8981\u628a\u5b83\u8d4b\u503c\u7ed9\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u5e76\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u5f20\u91cf\u3002 \u5728\u591aGPU\u4e0a\u6267\u884c\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u662f\u81ea\u7136\u800c\u7136\u7684\u4e8b\u3002 \u4f46\u662fPyTorch\u9ed8\u8ba4\u5c06\u53ea\u4f7f\u7528\u4e00\u4e2aGPU\u3002 \u4f7f\u7528 DataParallel \u53ef\u4ee5\u8f7b\u6613\u7684\u8ba9\u6a21\u578b\u5e76\u884c\u8fd0\u884c\u5728\u591a\u4e2aGPU\u4e0a\u3002 model = nn . DataParallel ( model ) \u8fd9\u624d\u662f\u8fd9\u7bc7\u6559\u7a0b\u7684\u6838\u5fc3\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u66f4\u8be6\u7ec6\u7684\u4ecb\u7ecd\u5b83\u3002","title":"\u6570\u636e\u5e76\u884c\uff08\u9009\u8bfb\uff09"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_2","text":"\u5bfc\u5165PyTorch\u6a21\u5757\u548c\u5b9a\u4e49\u53c2\u6570\u3002 import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader # Parameters and DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 Device device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" )","title":"\u5bfc\u5165\u548c\u53c2\u6570"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_3","text":"\u5236\u4f5c\u4e00\u4e2a\u865a\u62df\uff08\u968f\u673a\uff09\u6570\u636e\u96c6\uff0c \u4f60\u53ea\u9700\u5b9e\u73b0 __getitem__ class RandomDataset ( Dataset ): def __init__ ( self , size , length ): self . len = length self . data = torch . randn ( length , size ) def __getitem__ ( self , index ): return self . data [ index ] def __len__ ( self ): return self . len rand_loader = DataLoader ( dataset = RandomDataset ( input_size , data_size ), batch_size = batch_size , shuffle = True )","title":"\u865a\u62df\u6570\u636e\u96c6"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_4","text":"\u4f5c\u4e3a\u6f14\u793a\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\uff0c\u6267\u884c\u4e00\u4e2a\u7ebf\u6027\u64cd\u4f5c\uff0c\u7136\u540e\u5f97\u5230\u7ed3\u679c\u3002 \u8bf4\u660e\uff1a DataParallel \u80fd\u5728\u4efb\u4f55\u6a21\u578b\uff08CNN\uff0cRNN\uff0cCapsule Net\u7b49\uff09\u4e0a\u4f7f\u7528\u3002 \u6211\u4eec\u5728\u6a21\u578b\u5185\u90e8\u653e\u7f6e\u4e86\u4e00\u6761\u6253\u5370\u8bed\u53e5\u6765\u6253\u5370\u8f93\u5165\u548c\u8f93\u51fa\u5411\u91cf\u7684\u5927\u5c0f\u3002 \u8bf7\u6ce8\u610f\u6279\u6b21\u7684\u79e9\u4e3a0\u65f6\u6253\u5370\u7684\u5185\u5bb9\u3002 class Model ( nn . Module ): # Our model def __init__ ( self , input_size , output_size ): super ( Model , self ) . __init__ () self . fc = nn . Linear ( input_size , output_size ) def forward ( self , input ): output = self . fc ( input ) print ( \" \\t In Model: input size\" , input . size (), \"output size\" , output . size ()) return output","title":"\u7b80\u5355\u6a21\u578b"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_5","text":"\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u548c\u68c0\u6d4b\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002 \u5982\u679c\u6709\u591a\u4e2aGPU\uff0c\u4f7f\u7528 nn.DataParallel \u6765\u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002 \u7136\u540e\u901a\u8fc7m model.to(device) \u628a\u6a21\u578b\u653e\u5230GPU\u4e0a\u3002 model = Model ( input_size , output_size ) if torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn . DataParallel ( model ) model . to ( device ) Model( (fc): Linear(in_features=5, out_features=2, bias=True) )","title":"\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_6","text":"\u73b0\u5728\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 for data in rand_loader : input = data . to ( device ) output = model ( input ) print ( \"Outside: input size\" , input . size (), \"output_size\" , output . size ()) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"\u8fd0\u884c\u6a21\u578b"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_7","text":"\u5f53\u6ca1\u6709\u6216\u8005\u53ea\u6709\u4e00\u4e2aGPU\u65f6\uff0c\u5bf930\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\u8fdb\u884c\u6279\u5904\u7406\uff0c\u5f97\u5230\u4e86\u671f\u671b\u7684\u4e00\u6837\u5f97\u523030\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u4f46\u662f\u5982\u679c\u4f60\u6709\u591a\u4e2aGPU\uff0c\u4f60\u5f97\u5230\u5982\u4e0b\u7684\u7ed3\u679c\u3002 2 GPUs ~ If you have 2, you will see: .. code:: bash # on 2 GPUs Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3 GPUs ~ If you have 3 GPUs, you will see: .. code:: bash Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8 GPUs ~~ If you have 8, you will see: .. code:: bash Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"\u7ed3\u679c"},{"location":"tutorial/chapter01_getting-started/1_3_5_data_parallel_tutorial/#_8","text":"DataParallel\u4f1a\u81ea\u52a8\u7684\u5212\u5206\u6570\u636e\uff0c\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002 \u5e76\u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u4f5c\u4e1a\u540e\uff0c\u6536\u96c6\u5408\u5e76\u7ed3\u679c\u5e76\u8fd4\u56de\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u770b\u8fd9\u91cc\uff1a https://pytorch.org/tutorials/beginner/former _torchies/parallelism_tutorial.html.","title":"\u603b\u7ed3"},{"location":"tutorial/chapter01_getting-started/1_3_deep-learning-with-pytorch-60-minute-blitz/","text":"PyTorch \u6df1\u5ea6\u5b66\u4e60\uff1a60\u5206\u949f\u5feb\u901f\u5165\u95e8 \uff08\u5b98\u65b9\u4e2d\u6587\u7248\uff09 \u00b6 \u5b98\u65b9\u8fde\u63a5 Deep Learning with PyTorch: A 60 Minute Blitz \u76ee\u5f55 \u00b6 \u5f20\u91cf Tensor \u81ea\u52a8\u6c42\u5bfc Autograd \u795e\u7ecf\u7f51\u7edc Neural Networks \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 Classifier \u6570\u636e\u5e76\u884c Data Parallelism","title":"PyTorch \u6df1\u5ea6\u5b66\u4e60\uff1a60\u5206\u949f\u5feb\u901f\u5165\u95e8 \uff08\u5b98\u65b9\u4e2d\u6587\u7248\uff09"},{"location":"tutorial/chapter01_getting-started/1_3_deep-learning-with-pytorch-60-minute-blitz/#pytorch-60","text":"\u5b98\u65b9\u8fde\u63a5 Deep Learning with PyTorch: A 60 Minute Blitz","title":"PyTorch \u6df1\u5ea6\u5b66\u4e60\uff1a60\u5206\u949f\u5feb\u901f\u5165\u95e8 \uff08\u5b98\u65b9\u4e2d\u6587\u7248\uff09"},{"location":"tutorial/chapter01_getting-started/1_3_deep-learning-with-pytorch-60-minute-blitz/#_1","text":"\u5f20\u91cf Tensor \u81ea\u52a8\u6c42\u5bfc Autograd \u795e\u7ecf\u7f51\u7edc Neural Networks \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668 Classifier \u6570\u636e\u5e76\u884c Data Parallelism","title":"\u76ee\u5f55"},{"location":"tutorial/chapter02_basics/","text":"Pytorch \u624b\u518c\u7b2c\u4e8c\u7ae0 \uff1a \u57fa\u7840 \u00b6 \u76ee\u5f55 \u00b6 \u7b2c\u4e00\u8282 PyTorch \u57fa\u7840 \u00b6 \u5f20\u91cf \u81ea\u52a8\u6c42\u5bfc \u795e\u7ecf\u7f51\u7edc\u5305nn\u548c\u4f18\u5316\u5668optm \u6570\u636e\u7684\u52a0\u8f7d\u548c\u9884\u5904\u7406 \u7b2c\u4e8c\u8282 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u53ca\u6570\u5b66\u539f\u7406 \u00b6 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u53ca\u6570\u5b66\u539f\u7406 \u7b2c\u4e09\u8282 \u795e\u7ecf\u7f51\u7edc\u7b80\u4ecb \u00b6 \u795e\u7ecf\u7f51\u7edc\u7b80\u4ecb \u7b2c\u56db\u8282 \u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u00b6 \u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u7b2c\u4e94\u8282 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc \u00b6 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc","title":"Pytorch \u624b\u518c\u7b2c\u4e8c\u7ae0 \uff1a \u57fa\u7840"},{"location":"tutorial/chapter02_basics/#pytorch","text":"","title":"Pytorch \u624b\u518c\u7b2c\u4e8c\u7ae0 \uff1a \u57fa\u7840"},{"location":"tutorial/chapter02_basics/#_1","text":"","title":"\u76ee\u5f55"},{"location":"tutorial/chapter02_basics/#pytorch_1","text":"\u5f20\u91cf \u81ea\u52a8\u6c42\u5bfc \u795e\u7ecf\u7f51\u7edc\u5305nn\u548c\u4f18\u5316\u5668optm \u6570\u636e\u7684\u52a0\u8f7d\u548c\u9884\u5904\u7406","title":"\u7b2c\u4e00\u8282 PyTorch \u57fa\u7840"},{"location":"tutorial/chapter02_basics/#_2","text":"\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u53ca\u6570\u5b66\u539f\u7406","title":"\u7b2c\u4e8c\u8282 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u53ca\u6570\u5b66\u539f\u7406"},{"location":"tutorial/chapter02_basics/#_3","text":"\u795e\u7ecf\u7f51\u7edc\u7b80\u4ecb","title":"\u7b2c\u4e09\u8282 \u795e\u7ecf\u7f51\u7edc\u7b80\u4ecb"},{"location":"tutorial/chapter02_basics/#_4","text":"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","title":"\u7b2c\u56db\u8282 \u5377\u79ef\u795e\u7ecf\u7f51\u7edc"},{"location":"tutorial/chapter02_basics/#_5","text":"\u5faa\u73af\u795e\u7ecf\u7f51\u7edc","title":"\u7b2c\u4e94\u8282 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/","text":"PyTorch \u57fa\u7840 : \u5f20\u91cf \u00b6 \u5728\u7b2c\u4e00\u7ae0\u4e2d\u6211\u4eec\u5df2\u7ecf\u901a\u8fc7\u5b98\u65b9\u7684\u5165\u95e8\u6559\u7a0b\u5bf9PyTorch\u6709\u4e86\u4e00\u5b9a\u7684\u4e86\u89e3\uff0c\u8fd9\u4e00\u7ae0\u4f1a\u8be6\u7ec6\u4ecb\u7ecdPyTorch \u91cc\u9762\u7684\u57fa\u7840\u77e5\u8bc6\u3002 \u5168\u90e8\u638c\u63e1\u4e86\u8fd9\u4e9b\u57fa\u7840\u77e5\u8bc6\uff0c\u5728\u540e\u9762\u7684\u5e94\u7528\u4e2d\u624d\u80fd\u66f4\u52a0\u5feb\u901f\u8fdb\u9636\uff0c\u5982\u679c\u4f60\u5df2\u7ecf\u5bf9PyTorch\u6709\u4e00\u5b9a\u7684\u4e86\u89e3\uff0c\u53ef\u4ee5\u8df3\u8fc7\u6b64\u7ae0 # \u9996\u5148\u8981\u5f15\u5165\u76f8\u5173\u7684\u5305 import torch import numpy as np #\u6253\u5370\u4e00\u4e0b\u7248\u672c torch . __version__ '1.0.0' \u5f20\u91cf(Tensor) \u00b6 \u5f20\u91cf\u7684\u82f1\u6587\u662fTensor\uff0c\u5b83\u662fPyTorch\u91cc\u9762\u57fa\u7840\u7684\u8fd0\u7b97\u5355\u4f4d,\u4e0eNumpy\u7684ndarray\u76f8\u540c\u90fd\u8868\u793a\u7684\u662f\u4e00\u4e2a\u591a\u7ef4\u7684\u77e9\u9635\u3002 \u4e0endarray\u7684\u6700\u5927\u533a\u522b\u5c31\u662f\uff0cPyTorch\u7684Tensor\u53ef\u4ee5\u5728 GPU \u4e0a\u8fd0\u884c\uff0c\u800c numpy \u7684 ndarray \u53ea\u80fd\u5728 CPU \u4e0a\u8fd0\u884c\uff0c\u5728GPU\u4e0a\u8fd0\u884c\u5927\u5927\u52a0\u5feb\u4e86\u8fd0\u7b97\u901f\u5ea6\u3002 \u4e0b\u9762\u6211\u4eec\u751f\u6210\u4e00\u4e2a\u7b80\u5355\u7684\u5f20\u91cf x = torch . rand ( 2 , 3 ) x tensor([[0.6904, 0.7419, 0.8010], [0.1722, 0.2442, 0.8181]]) \u4ee5\u4e0a\u751f\u6210\u4e86\u4e00\u4e2a\uff0c2\u884c3\u5217\u7684\u7684\u77e9\u9635\uff0c\u6211\u4eec\u770b\u4e00\u4e0b\u4ed6\u7684\u5927\u5c0f\uff1a # \u53ef\u4ee5\u4f7f\u7528\u4e0enumpy\u76f8\u540c\u7684shape\u5c5e\u6027\u67e5\u770b print ( x . shape ) # \u4e5f\u53ef\u4ee5\u4f7f\u7528size()\u51fd\u6570\uff0c\u8fd4\u56de\u7684\u7ed3\u679c\u90fd\u662f\u76f8\u540c\u7684 print ( x . size ()) torch.Size([2, 3]) torch.Size([2, 3]) \u5f20\u91cf\uff08Tensor\uff09\u662f\u4e00\u4e2a\u5b9a\u4e49\u5728\u4e00\u4e9b\u5411\u91cf\u7a7a\u95f4\u548c\u4e00\u4e9b\u5bf9\u5076\u7a7a\u95f4\u7684\u7b1b\u5361\u513f\u79ef\u4e0a\u7684\u591a\u91cd\u7ebf\u6027\u6620\u5c04\uff0c\u5176\u5750\u6807\u662f|n|\u7ef4\u7a7a\u95f4\u5185\uff0c\u6709|n|\u4e2a\u5206\u91cf\u7684\u4e00\u79cd\u91cf\uff0c \u5176\u4e2d\u6bcf\u4e2a\u5206\u91cf\u90fd\u662f\u5750\u6807\u7684\u51fd\u6570\uff0c \u800c\u5728\u5750\u6807\u53d8\u6362\u65f6\uff0c\u8fd9\u4e9b\u5206\u91cf\u4e5f\u4f9d\u7167\u67d0\u4e9b\u89c4\u5219\u4f5c\u7ebf\u6027\u53d8\u6362\u3002r\u79f0\u4e3a\u8be5\u5f20\u91cf\u7684\u79e9\u6216\u9636\uff08\u4e0e\u77e9\u9635\u7684\u79e9\u548c\u9636\u5747\u65e0\u5173\u7cfb\uff09\u3002 (\u6765\u81ea\u767e\u5ea6\u767e\u79d1) \u4e0b\u9762\u6211\u4eec\u6765\u751f\u6210\u4e00\u4e9b\u591a\u7ef4\u7684\u5f20\u91cf\uff1a y = torch . rand ( 2 , 3 , 4 , 5 ) print ( y . size ()) y torch.Size([2, 3, 4, 5]) tensor([[[[0.9071, 0.0616, 0.0006, 0.6031, 0.0714], [0.6592, 0.9700, 0.0253, 0.0726, 0.5360], [0.5416, 0.1138, 0.9592, 0.6779, 0.6501], [0.0546, 0.8287, 0.7748, 0.4352, 0.9232]], [[0.0730, 0.4228, 0.7407, 0.4099, 0.1482], [0.5408, 0.9156, 0.6554, 0.5787, 0.9775], [0.4262, 0.3644, 0.1993, 0.4143, 0.5757], [0.9307, 0.8839, 0.8462, 0.0933, 0.6688]], [[0.4447, 0.0929, 0.9882, 0.5392, 0.1159], [0.4790, 0.5115, 0.4005, 0.9486, 0.0054], [0.8955, 0.8097, 0.1227, 0.2250, 0.5830], [0.8483, 0.2070, 0.1067, 0.4727, 0.5095]]], [[[0.9438, 0.2601, 0.2885, 0.5457, 0.7528], [0.2971, 0.2171, 0.3910, 0.1924, 0.2570], [0.7491, 0.9749, 0.2703, 0.2198, 0.9472], [0.1216, 0.6647, 0.8809, 0.0125, 0.5513]], [[0.0870, 0.6622, 0.7252, 0.4783, 0.0160], [0.7832, 0.6050, 0.7469, 0.7947, 0.8052], [0.1755, 0.4489, 0.0602, 0.8073, 0.3028], [0.9937, 0.6780, 0.9425, 0.0059, 0.0451]], [[0.3851, 0.8742, 0.5932, 0.4899, 0.8354], [0.8577, 0.3705, 0.0229, 0.7097, 0.7557], [0.1505, 0.3527, 0.0843, 0.0088, 0.8741], [0.6041, 0.8797, 0.6189, 0.9495, 0.1479]]]]) \u5728\u540c\u6784\u7684\u610f\u4e49\u4e0b\uff0c\u7b2c\u96f6\u9636\u5f20\u91cf \uff08r = 0\uff09 \u4e3a\u6807\u91cf \uff08Scalar\uff09\uff0c\u7b2c\u4e00\u9636\u5f20\u91cf \uff08r = 1\uff09 \u4e3a\u5411\u91cf \uff08Vector\uff09\uff0c \u7b2c\u4e8c\u9636\u5f20\u91cf \uff08r = 2\uff09 \u5219\u6210\u4e3a\u77e9\u9635 \uff08Matrix\uff09\uff0c\u7b2c\u4e09\u9636\u4ee5\u4e0a\u7684\u7edf\u79f0\u4e3a\u591a\u7ef4\u5f20\u91cf\u3002 \u5176\u4e2d\u8981\u7279\u522b\u6ce8\u610f\u7684\u5c31\u662f\u6807\u91cf\uff0c\u6211\u4eec\u5148\u751f\u6210\u4e00\u4e2a\u6807\u91cf\uff1a #\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u73b0\u6709\u6570\u5b57\u751f\u6210 scalar = torch . tensor ( 3.1433223 ) print ( scalar ) #\u6253\u5370\u6807\u91cf\u7684\u5927\u5c0f scalar . size () tensor(3.1433) torch.Size([]) \u5bf9\u4e8e\u6807\u91cf\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 .item() \u4ece\u4e2d\u53d6\u51fa\u5176\u5bf9\u5e94\u7684python\u5bf9\u8c61\u7684\u6570\u503c scalar . item () 3.143322229385376 \u7279\u522b\u7684\uff1a\u5982\u679c\u5f20\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684tensor\u4e5f\u53ef\u4ee5\u8c03\u7528 tensor.item \u65b9\u6cd5 tensor = torch . tensor ([ 3.1433223 ]) print ( tensor ) tensor . size () tensor([3.1433]) torch.Size([1]) tensor . item () 3.143322229385376 \u57fa\u672c\u7c7b\u578b \u00b6 Tensor\u7684\u57fa\u672c\u6570\u636e\u7c7b\u578b\u6709\u4e94\u79cd\uff1a - 32\u4f4d\u6d6e\u70b9\u578b\uff1atorch.FloatTensor\u3002 (\u9ed8\u8ba4) - 64\u4f4d\u6574\u578b\uff1atorch.LongTensor\u3002 - 32\u4f4d\u6574\u578b\uff1atorch.IntTensor\u3002 - 16\u4f4d\u6574\u578b\uff1atorch.ShortTensor\u3002 - 64\u4f4d\u6d6e\u70b9\u578b\uff1atorch.DoubleTensor\u3002 \u9664\u4ee5\u4e0a\u6570\u5b57\u7c7b\u578b\u5916\uff0c\u8fd8\u6709 byte\u548cchart\u578b long = tensor . long () long tensor([3]) half = tensor . half () half tensor([3.1426], dtype=torch.float16) int_t = tensor . int () int_t tensor([3], dtype=torch.int32) flo = tensor . float () flo tensor([3.1433]) short = tensor . short () short tensor([3], dtype=torch.int16) ch = tensor . char () ch tensor([3], dtype=torch.int8) bt = tensor . byte () bt tensor([3], dtype=torch.uint8) Numpy\u8f6c\u6362 \u00b6 \u4f7f\u7528numpy\u65b9\u6cd5\u5c06Tensor\u8f6c\u4e3andarray a = torch . randn (( 3 , 2 )) # tensor\u8f6c\u5316\u4e3anumpy numpy_a = a . numpy () print ( numpy_a ) [[ 0.46819344 1.3774964 ] [ 0.9491934 1.4543315 ] [-0.42792308 0.99790514]] numpy\u8f6c\u5316\u4e3aTensor torch_a = torch . from_numpy ( numpy_a ) torch_a tensor([[ 0.4682, 1.3775], [ 0.9492, 1.4543], [-0.4279, 0.9979]]) Tensor\u548cnumpy\u5bf9\u8c61\u5171\u4eab\u5185\u5b58\uff0c\u6240\u4ee5\u4ed6\u4eec\u4e4b\u95f4\u7684\u8f6c\u6362\u5f88\u5feb\uff0c\u800c\u4e14\u51e0\u4e4e\u4e0d\u4f1a\u6d88\u8017\u4ec0\u4e48\u8d44\u6e90\u3002\u4f46\u8fd9\u4e5f\u610f\u5473\u7740\uff0c\u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u53d8\u4e86\uff0c\u53e6\u5916\u4e00\u4e2a\u4e5f\u4f1a\u968f\u4e4b\u6539\u53d8\u3002 \u8bbe\u5907\u95f4\u8f6c\u6362 \u00b6 \u4e00\u822c\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4f7f\u7528.cuda\u65b9\u6cd5\u5c06tensor\u79fb\u52a8\u5230gpu\uff0c\u8fd9\u6b65\u64cd\u4f5c\u9700\u8981cuda\u8bbe\u5907\u652f\u6301 cpu_a = torch . rand ( 4 , 3 ) cpu_a . type () 'torch.FloatTensor' gpu_a = cpu_a . cuda () gpu_a . type () 'torch.cuda.FloatTensor' \u4f7f\u7528.cpu\u65b9\u6cd5\u5c06tensor\u79fb\u52a8\u5230cpu cpu_b = gpu_a . cpu () cpu_b . type () 'torch.FloatTensor' \u5982\u679c\u6211\u4eec\u6709\u591aGPU\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u4f7f\u7528to\u65b9\u6cd5\u6765\u786e\u5b9a\u4f7f\u7528\u90a3\u4e2a\u8bbe\u5907\uff0c\u8fd9\u91cc\u53ea\u505a\u4e2a\u7b80\u5355\u7684\u5b9e\u4f8b\uff1a #\u4f7f\u7528torch.cuda.is_available()\u6765\u786e\u5b9a\u662f\u5426\u6709cuda\u8bbe\u5907 device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) print ( device ) #\u5c06tensor\u4f20\u9001\u5230\u8bbe\u5907 gpu_b = cpu_b . to ( device ) gpu_b . type () cuda 'torch.cuda.FloatTensor' \u521d\u59cb\u5316 \u00b6 Pytorch\u4e2d\u6709\u8bb8\u591a\u9ed8\u8ba4\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528 # \u4f7f\u7528[0,1]\u5747\u5300\u5206\u5e03\u968f\u673a\u521d\u59cb\u5316\u4e8c\u7ef4\u6570\u7ec4 rnd = torch . rand ( 5 , 3 ) rnd tensor([[0.3804, 0.0297, 0.5241], [0.4111, 0.8887, 0.4642], [0.7302, 0.5913, 0.7182], [0.3048, 0.8055, 0.2176], [0.6195, 0.1620, 0.7726]]) ##\u521d\u59cb\u5316\uff0c\u4f7f\u75281\u586b\u5145 one = torch . ones ( 2 , 2 ) one tensor([[1., 1.], [1., 1.]]) ##\u521d\u59cb\u5316\uff0c\u4f7f\u75280\u586b\u5145 zero = torch . zeros ( 2 , 2 ) zero tensor([[0., 0.], [0., 0.]]) #\u521d\u59cb\u5316\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\uff0c\u5373\u5bf9\u89d2\u7ebf\u4e3a1 \u5176\u4ed6\u4e3a0 eye = torch . eye ( 2 , 2 ) eye tensor([[1., 0.], [0., 1.]]) \u5e38\u7528\u65b9\u6cd5 \u00b6 PyTorch\u4e2d\u5bf9\u5f20\u91cf\u7684\u64cd\u4f5capi \u548c NumPy \u975e\u5e38\u76f8\u4f3c\uff0c\u5982\u679c\u719f\u6089 NumPy \u4e2d\u7684\u64cd\u4f5c\uff0c\u90a3\u4e48 \u4ed6\u4eec\u4e8c\u8005 \u57fa\u672c\u662f\u4e00\u81f4\u7684\uff1a x = torch . randn ( 3 , 3 ) print ( x ) tensor([[ 0.6922, -0.4824, 0.8594], [ 0.4509, -0.8155, -0.0368], [ 1.3533, 0.5545, -0.0509]]) # \u6cbf\u7740\u884c\u53d6\u6700\u5927\u503c max_value , max_idx = torch . max ( x , dim = 1 ) print ( max_value , max_idx ) tensor([0.8594, 0.4509, 1.3533]) tensor([2, 0, 0]) # \u6bcf\u884c x \u6c42\u548c sum_x = torch . sum ( x , dim = 1 ) print ( sum_x ) tensor([ 1.0692, -0.4014, 1.8568]) y = torch . randn ( 3 , 3 ) z = x + y print ( z ) tensor([[-0.3821, -2.6932, -1.3884], [ 0.7468, -0.7697, -0.0883], [ 0.7688, -1.3485, 0.7517]]) \u6b63\u5982\u5b98\u65b960\u5206\u949f\u6559\u7a0b\u4e2d\u6240\u8bf4\uff0c\u4ee5_\u4e3a\u7ed3\u5c3e\u7684\uff0c\u5747\u4f1a\u6539\u53d8\u8c03\u7528\u503c # add \u5b8c\u6210\u540ex\u7684\u503c\u6539\u53d8\u4e86 x . add_ ( y ) print ( x ) tensor([[-0.3821, -2.6932, -1.3884], [ 0.7468, -0.7697, -0.0883], [ 0.7688, -1.3485, 0.7517]]) \u5f20\u91cf\u7684\u57fa\u672c\u64cd\u4f5c\u90fd\u4ecb\u7ecd\u7684\u7684\u5dee\u4e0d\u591a\u4e86\uff0c\u4e0b\u4e00\u7ae0\u4ecb\u7ecdPyTorch\u7684\u81ea\u52a8\u6c42\u5bfc\u673a\u5236","title":"2.1 Tensor"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#pytorch","text":"\u5728\u7b2c\u4e00\u7ae0\u4e2d\u6211\u4eec\u5df2\u7ecf\u901a\u8fc7\u5b98\u65b9\u7684\u5165\u95e8\u6559\u7a0b\u5bf9PyTorch\u6709\u4e86\u4e00\u5b9a\u7684\u4e86\u89e3\uff0c\u8fd9\u4e00\u7ae0\u4f1a\u8be6\u7ec6\u4ecb\u7ecdPyTorch \u91cc\u9762\u7684\u57fa\u7840\u77e5\u8bc6\u3002 \u5168\u90e8\u638c\u63e1\u4e86\u8fd9\u4e9b\u57fa\u7840\u77e5\u8bc6\uff0c\u5728\u540e\u9762\u7684\u5e94\u7528\u4e2d\u624d\u80fd\u66f4\u52a0\u5feb\u901f\u8fdb\u9636\uff0c\u5982\u679c\u4f60\u5df2\u7ecf\u5bf9PyTorch\u6709\u4e00\u5b9a\u7684\u4e86\u89e3\uff0c\u53ef\u4ee5\u8df3\u8fc7\u6b64\u7ae0 # \u9996\u5148\u8981\u5f15\u5165\u76f8\u5173\u7684\u5305 import torch import numpy as np #\u6253\u5370\u4e00\u4e0b\u7248\u672c torch . __version__ '1.0.0'","title":"PyTorch \u57fa\u7840 : \u5f20\u91cf"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#tensor","text":"\u5f20\u91cf\u7684\u82f1\u6587\u662fTensor\uff0c\u5b83\u662fPyTorch\u91cc\u9762\u57fa\u7840\u7684\u8fd0\u7b97\u5355\u4f4d,\u4e0eNumpy\u7684ndarray\u76f8\u540c\u90fd\u8868\u793a\u7684\u662f\u4e00\u4e2a\u591a\u7ef4\u7684\u77e9\u9635\u3002 \u4e0endarray\u7684\u6700\u5927\u533a\u522b\u5c31\u662f\uff0cPyTorch\u7684Tensor\u53ef\u4ee5\u5728 GPU \u4e0a\u8fd0\u884c\uff0c\u800c numpy \u7684 ndarray \u53ea\u80fd\u5728 CPU \u4e0a\u8fd0\u884c\uff0c\u5728GPU\u4e0a\u8fd0\u884c\u5927\u5927\u52a0\u5feb\u4e86\u8fd0\u7b97\u901f\u5ea6\u3002 \u4e0b\u9762\u6211\u4eec\u751f\u6210\u4e00\u4e2a\u7b80\u5355\u7684\u5f20\u91cf x = torch . rand ( 2 , 3 ) x tensor([[0.6904, 0.7419, 0.8010], [0.1722, 0.2442, 0.8181]]) \u4ee5\u4e0a\u751f\u6210\u4e86\u4e00\u4e2a\uff0c2\u884c3\u5217\u7684\u7684\u77e9\u9635\uff0c\u6211\u4eec\u770b\u4e00\u4e0b\u4ed6\u7684\u5927\u5c0f\uff1a # \u53ef\u4ee5\u4f7f\u7528\u4e0enumpy\u76f8\u540c\u7684shape\u5c5e\u6027\u67e5\u770b print ( x . shape ) # \u4e5f\u53ef\u4ee5\u4f7f\u7528size()\u51fd\u6570\uff0c\u8fd4\u56de\u7684\u7ed3\u679c\u90fd\u662f\u76f8\u540c\u7684 print ( x . size ()) torch.Size([2, 3]) torch.Size([2, 3]) \u5f20\u91cf\uff08Tensor\uff09\u662f\u4e00\u4e2a\u5b9a\u4e49\u5728\u4e00\u4e9b\u5411\u91cf\u7a7a\u95f4\u548c\u4e00\u4e9b\u5bf9\u5076\u7a7a\u95f4\u7684\u7b1b\u5361\u513f\u79ef\u4e0a\u7684\u591a\u91cd\u7ebf\u6027\u6620\u5c04\uff0c\u5176\u5750\u6807\u662f|n|\u7ef4\u7a7a\u95f4\u5185\uff0c\u6709|n|\u4e2a\u5206\u91cf\u7684\u4e00\u79cd\u91cf\uff0c \u5176\u4e2d\u6bcf\u4e2a\u5206\u91cf\u90fd\u662f\u5750\u6807\u7684\u51fd\u6570\uff0c \u800c\u5728\u5750\u6807\u53d8\u6362\u65f6\uff0c\u8fd9\u4e9b\u5206\u91cf\u4e5f\u4f9d\u7167\u67d0\u4e9b\u89c4\u5219\u4f5c\u7ebf\u6027\u53d8\u6362\u3002r\u79f0\u4e3a\u8be5\u5f20\u91cf\u7684\u79e9\u6216\u9636\uff08\u4e0e\u77e9\u9635\u7684\u79e9\u548c\u9636\u5747\u65e0\u5173\u7cfb\uff09\u3002 (\u6765\u81ea\u767e\u5ea6\u767e\u79d1) \u4e0b\u9762\u6211\u4eec\u6765\u751f\u6210\u4e00\u4e9b\u591a\u7ef4\u7684\u5f20\u91cf\uff1a y = torch . rand ( 2 , 3 , 4 , 5 ) print ( y . size ()) y torch.Size([2, 3, 4, 5]) tensor([[[[0.9071, 0.0616, 0.0006, 0.6031, 0.0714], [0.6592, 0.9700, 0.0253, 0.0726, 0.5360], [0.5416, 0.1138, 0.9592, 0.6779, 0.6501], [0.0546, 0.8287, 0.7748, 0.4352, 0.9232]], [[0.0730, 0.4228, 0.7407, 0.4099, 0.1482], [0.5408, 0.9156, 0.6554, 0.5787, 0.9775], [0.4262, 0.3644, 0.1993, 0.4143, 0.5757], [0.9307, 0.8839, 0.8462, 0.0933, 0.6688]], [[0.4447, 0.0929, 0.9882, 0.5392, 0.1159], [0.4790, 0.5115, 0.4005, 0.9486, 0.0054], [0.8955, 0.8097, 0.1227, 0.2250, 0.5830], [0.8483, 0.2070, 0.1067, 0.4727, 0.5095]]], [[[0.9438, 0.2601, 0.2885, 0.5457, 0.7528], [0.2971, 0.2171, 0.3910, 0.1924, 0.2570], [0.7491, 0.9749, 0.2703, 0.2198, 0.9472], [0.1216, 0.6647, 0.8809, 0.0125, 0.5513]], [[0.0870, 0.6622, 0.7252, 0.4783, 0.0160], [0.7832, 0.6050, 0.7469, 0.7947, 0.8052], [0.1755, 0.4489, 0.0602, 0.8073, 0.3028], [0.9937, 0.6780, 0.9425, 0.0059, 0.0451]], [[0.3851, 0.8742, 0.5932, 0.4899, 0.8354], [0.8577, 0.3705, 0.0229, 0.7097, 0.7557], [0.1505, 0.3527, 0.0843, 0.0088, 0.8741], [0.6041, 0.8797, 0.6189, 0.9495, 0.1479]]]]) \u5728\u540c\u6784\u7684\u610f\u4e49\u4e0b\uff0c\u7b2c\u96f6\u9636\u5f20\u91cf \uff08r = 0\uff09 \u4e3a\u6807\u91cf \uff08Scalar\uff09\uff0c\u7b2c\u4e00\u9636\u5f20\u91cf \uff08r = 1\uff09 \u4e3a\u5411\u91cf \uff08Vector\uff09\uff0c \u7b2c\u4e8c\u9636\u5f20\u91cf \uff08r = 2\uff09 \u5219\u6210\u4e3a\u77e9\u9635 \uff08Matrix\uff09\uff0c\u7b2c\u4e09\u9636\u4ee5\u4e0a\u7684\u7edf\u79f0\u4e3a\u591a\u7ef4\u5f20\u91cf\u3002 \u5176\u4e2d\u8981\u7279\u522b\u6ce8\u610f\u7684\u5c31\u662f\u6807\u91cf\uff0c\u6211\u4eec\u5148\u751f\u6210\u4e00\u4e2a\u6807\u91cf\uff1a #\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u73b0\u6709\u6570\u5b57\u751f\u6210 scalar = torch . tensor ( 3.1433223 ) print ( scalar ) #\u6253\u5370\u6807\u91cf\u7684\u5927\u5c0f scalar . size () tensor(3.1433) torch.Size([]) \u5bf9\u4e8e\u6807\u91cf\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 .item() \u4ece\u4e2d\u53d6\u51fa\u5176\u5bf9\u5e94\u7684python\u5bf9\u8c61\u7684\u6570\u503c scalar . item () 3.143322229385376 \u7279\u522b\u7684\uff1a\u5982\u679c\u5f20\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684tensor\u4e5f\u53ef\u4ee5\u8c03\u7528 tensor.item \u65b9\u6cd5 tensor = torch . tensor ([ 3.1433223 ]) print ( tensor ) tensor . size () tensor([3.1433]) torch.Size([1]) tensor . item () 3.143322229385376","title":"\u5f20\u91cf(Tensor)"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#_1","text":"Tensor\u7684\u57fa\u672c\u6570\u636e\u7c7b\u578b\u6709\u4e94\u79cd\uff1a - 32\u4f4d\u6d6e\u70b9\u578b\uff1atorch.FloatTensor\u3002 (\u9ed8\u8ba4) - 64\u4f4d\u6574\u578b\uff1atorch.LongTensor\u3002 - 32\u4f4d\u6574\u578b\uff1atorch.IntTensor\u3002 - 16\u4f4d\u6574\u578b\uff1atorch.ShortTensor\u3002 - 64\u4f4d\u6d6e\u70b9\u578b\uff1atorch.DoubleTensor\u3002 \u9664\u4ee5\u4e0a\u6570\u5b57\u7c7b\u578b\u5916\uff0c\u8fd8\u6709 byte\u548cchart\u578b long = tensor . long () long tensor([3]) half = tensor . half () half tensor([3.1426], dtype=torch.float16) int_t = tensor . int () int_t tensor([3], dtype=torch.int32) flo = tensor . float () flo tensor([3.1433]) short = tensor . short () short tensor([3], dtype=torch.int16) ch = tensor . char () ch tensor([3], dtype=torch.int8) bt = tensor . byte () bt tensor([3], dtype=torch.uint8)","title":"\u57fa\u672c\u7c7b\u578b"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#numpy","text":"\u4f7f\u7528numpy\u65b9\u6cd5\u5c06Tensor\u8f6c\u4e3andarray a = torch . randn (( 3 , 2 )) # tensor\u8f6c\u5316\u4e3anumpy numpy_a = a . numpy () print ( numpy_a ) [[ 0.46819344 1.3774964 ] [ 0.9491934 1.4543315 ] [-0.42792308 0.99790514]] numpy\u8f6c\u5316\u4e3aTensor torch_a = torch . from_numpy ( numpy_a ) torch_a tensor([[ 0.4682, 1.3775], [ 0.9492, 1.4543], [-0.4279, 0.9979]]) Tensor\u548cnumpy\u5bf9\u8c61\u5171\u4eab\u5185\u5b58\uff0c\u6240\u4ee5\u4ed6\u4eec\u4e4b\u95f4\u7684\u8f6c\u6362\u5f88\u5feb\uff0c\u800c\u4e14\u51e0\u4e4e\u4e0d\u4f1a\u6d88\u8017\u4ec0\u4e48\u8d44\u6e90\u3002\u4f46\u8fd9\u4e5f\u610f\u5473\u7740\uff0c\u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u53d8\u4e86\uff0c\u53e6\u5916\u4e00\u4e2a\u4e5f\u4f1a\u968f\u4e4b\u6539\u53d8\u3002","title":"Numpy\u8f6c\u6362"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#_2","text":"\u4e00\u822c\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4f7f\u7528.cuda\u65b9\u6cd5\u5c06tensor\u79fb\u52a8\u5230gpu\uff0c\u8fd9\u6b65\u64cd\u4f5c\u9700\u8981cuda\u8bbe\u5907\u652f\u6301 cpu_a = torch . rand ( 4 , 3 ) cpu_a . type () 'torch.FloatTensor' gpu_a = cpu_a . cuda () gpu_a . type () 'torch.cuda.FloatTensor' \u4f7f\u7528.cpu\u65b9\u6cd5\u5c06tensor\u79fb\u52a8\u5230cpu cpu_b = gpu_a . cpu () cpu_b . type () 'torch.FloatTensor' \u5982\u679c\u6211\u4eec\u6709\u591aGPU\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u4f7f\u7528to\u65b9\u6cd5\u6765\u786e\u5b9a\u4f7f\u7528\u90a3\u4e2a\u8bbe\u5907\uff0c\u8fd9\u91cc\u53ea\u505a\u4e2a\u7b80\u5355\u7684\u5b9e\u4f8b\uff1a #\u4f7f\u7528torch.cuda.is_available()\u6765\u786e\u5b9a\u662f\u5426\u6709cuda\u8bbe\u5907 device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) print ( device ) #\u5c06tensor\u4f20\u9001\u5230\u8bbe\u5907 gpu_b = cpu_b . to ( device ) gpu_b . type () cuda 'torch.cuda.FloatTensor'","title":"\u8bbe\u5907\u95f4\u8f6c\u6362"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#_3","text":"Pytorch\u4e2d\u6709\u8bb8\u591a\u9ed8\u8ba4\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528 # \u4f7f\u7528[0,1]\u5747\u5300\u5206\u5e03\u968f\u673a\u521d\u59cb\u5316\u4e8c\u7ef4\u6570\u7ec4 rnd = torch . rand ( 5 , 3 ) rnd tensor([[0.3804, 0.0297, 0.5241], [0.4111, 0.8887, 0.4642], [0.7302, 0.5913, 0.7182], [0.3048, 0.8055, 0.2176], [0.6195, 0.1620, 0.7726]]) ##\u521d\u59cb\u5316\uff0c\u4f7f\u75281\u586b\u5145 one = torch . ones ( 2 , 2 ) one tensor([[1., 1.], [1., 1.]]) ##\u521d\u59cb\u5316\uff0c\u4f7f\u75280\u586b\u5145 zero = torch . zeros ( 2 , 2 ) zero tensor([[0., 0.], [0., 0.]]) #\u521d\u59cb\u5316\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\uff0c\u5373\u5bf9\u89d2\u7ebf\u4e3a1 \u5176\u4ed6\u4e3a0 eye = torch . eye ( 2 , 2 ) eye tensor([[1., 0.], [0., 1.]])","title":"\u521d\u59cb\u5316"},{"location":"tutorial/chapter02_basics/2_1_pytorch-basics-tensor/#_4","text":"PyTorch\u4e2d\u5bf9\u5f20\u91cf\u7684\u64cd\u4f5capi \u548c NumPy \u975e\u5e38\u76f8\u4f3c\uff0c\u5982\u679c\u719f\u6089 NumPy \u4e2d\u7684\u64cd\u4f5c\uff0c\u90a3\u4e48 \u4ed6\u4eec\u4e8c\u8005 \u57fa\u672c\u662f\u4e00\u81f4\u7684\uff1a x = torch . randn ( 3 , 3 ) print ( x ) tensor([[ 0.6922, -0.4824, 0.8594], [ 0.4509, -0.8155, -0.0368], [ 1.3533, 0.5545, -0.0509]]) # \u6cbf\u7740\u884c\u53d6\u6700\u5927\u503c max_value , max_idx = torch . max ( x , dim = 1 ) print ( max_value , max_idx ) tensor([0.8594, 0.4509, 1.3533]) tensor([2, 0, 0]) # \u6bcf\u884c x \u6c42\u548c sum_x = torch . sum ( x , dim = 1 ) print ( sum_x ) tensor([ 1.0692, -0.4014, 1.8568]) y = torch . randn ( 3 , 3 ) z = x + y print ( z ) tensor([[-0.3821, -2.6932, -1.3884], [ 0.7468, -0.7697, -0.0883], [ 0.7688, -1.3485, 0.7517]]) \u6b63\u5982\u5b98\u65b960\u5206\u949f\u6559\u7a0b\u4e2d\u6240\u8bf4\uff0c\u4ee5_\u4e3a\u7ed3\u5c3e\u7684\uff0c\u5747\u4f1a\u6539\u53d8\u8c03\u7528\u503c # add \u5b8c\u6210\u540ex\u7684\u503c\u6539\u53d8\u4e86 x . add_ ( y ) print ( x ) tensor([[-0.3821, -2.6932, -1.3884], [ 0.7468, -0.7697, -0.0883], [ 0.7688, -1.3485, 0.7517]]) \u5f20\u91cf\u7684\u57fa\u672c\u64cd\u4f5c\u90fd\u4ecb\u7ecd\u7684\u7684\u5dee\u4e0d\u591a\u4e86\uff0c\u4e0b\u4e00\u7ae0\u4ecb\u7ecdPyTorch\u7684\u81ea\u52a8\u6c42\u5bfc\u673a\u5236","title":"\u5e38\u7528\u65b9\u6cd5"}]}